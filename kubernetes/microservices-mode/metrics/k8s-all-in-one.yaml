apiVersion: v1
kind: Namespace
metadata:
  labels:
    team: team-infra
  name: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: prometheus-blackbox-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-blackbox-exporter
    app.kubernetes.io/version: v0.24.0
    helm.sh/chart: prometheus-blackbox-exporter-8.4.0
    team: team-infra
  name: prometheus-blackbox-exporter
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: prometheus-node-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/version: 1.6.1
    helm.sh/chart: prometheus-node-exporter-4.23.2
    team: team-infra
  name: prometheus-node-exporter
  namespace: monitoring-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
rules:
- apiGroups:
  - ""
  - discovery.k8s.io
  - networking.k8s.io
  resources:
  - endpoints
  - endpointslices
  - ingresses
  - nodes
  - nodes/proxy
  - nodes/metrics
  - pods
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  - pods/log
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.grafana.com
  resources:
  - podlogs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheusrules
  verbs:
  - get
  - list
  - watch
- nonResourceURLs:
  - /metrics
  verbs:
  - get
- apiGroups:
  - monitoring.coreos.com
  resources:
  - podmonitors
  - servicemonitors
  - probes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: grafana-agent
subjects:
- kind: ServiceAccount
  name: grafana-agent
  namespace: monitoring-system
---
apiVersion: v1
data:
  metrics.river: "logging {\n  level  = coalesce(env(\"AGENT_LOG_LEVEL\"), \"info\")\n
    \ format = \"logfmt\"\n}\n\nmodule.file \"lgtmp\" {\n\tfilename = env(\"AGENT_CONFIG_FOLDER\")
    + \"/lgtmp.river\"\n\n\targuments {\n    cluster          = coalesce(env(\"CLUSTER\"),
    \"k3d-k3s-codelab\")\n    metrics_endpoint = coalesce(env(\"METRICS_ENDPOINT\"),
    \"http://nginx.monitoring-system:8080\")\n\t}\n}\n\nmodule.file \"metrics_primary\"
    {\n\tfilename = env(\"AGENT_CONFIG_FOLDER\") + \"/metrics-all.river\"\n\n  arguments
    {\n    forward_to = [module.file.lgtmp.exports.metrics_receiver]\n    clustering
    = true\n    scrape_port_named_metrics = true\n  }\n}"
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: agent-metrics-config-t5d9gg7fc6
  namespace: monitoring-system
---
apiVersion: v1
data:
  auto-scrape-endpoints.river: |
    /*
    Module: scrape-endpoints
    Description:
      Kubernetes Service Endpoints Auto-Scraping
      ------------------------------------------------------------------------------------------------------------------------------------
      Each port attached to an endpoint is an eligible target, oftentimes endpoint will have multiple ports.
      There may be instances when you want to scrape all ports or some ports and not others. To support this
      the following annotations are available:

      only scrape endpoints with scrape set to true, this can be single valued i.e. scrape all ports for
      the endpoint:

      metrics.agent.grafana.com/scrape: true
      prometheus.io/scrape: true

      the default scraping scheme is http, this can be specified as a single value which would override,
      the schema being used for all ports attached to the endpoint:

      metrics.agent.grafana.com/scheme: https
      prometheus.io/scheme: https

      the default path to scrape is /metrics, this can be specified as a single value which would override,
      the scrape path being used for all ports attached to the endpoint:

      metrics.agent.grafana.com/path: /metrics/some_path
      prometheus.io/path: /metrics/some_path

      the default port to scrape is the endpoint port, this can be specified as a single value which would
      override the scrape port being used for all ports attached to the endpoint, note that even if aan endpoint had
      multiple targets, the relabel_config targets are deduped before scraping:

      metrics.agent.grafana.com/port: 8080
      prometheus.io/port: 8080

      the default interval to scrape is 1m, this can be specified as a single value which would override,
      the scrape interval being used for all ports attached to the endpoint:

      metrics.agent.grafana.com/interval: 5m
      prometheus.io/interval: 5m

      the default timeout for scraping is 10s, this can be specified as a single value which would override,
      the scrape interval being used for all ports attached to the endpoint:

      metrics.agent.grafana.com/timeout: 30s
      prometheus.io/timeout: 30s
    */
    argument "forward_to" {
      // comment = "Must be a list(MetricssReceiver) where collected logs should be forwarded to"
      optional = false
    }

    argument "tenant" {
      // comment = "The tenant to filter logs to.  This does not have to be the tenantId, this is the value to look for in the logs.agent.grafana.com/tenant annotation, and this can be a regex."
      optional = true
      default = ".*"
    }

    argument "scrape_port_named_metrics" {
      // comment = "Whether or not to automatically scrape endpoints that have a port with 'metrics' in the name"
      optional = true
      default = false
    }

    argument "clustering" {
      // comment = "Whether or not clustering should be enabled"
      optional = true
      default = false
    }

    argument "git_repo" {
      optional = true
      default = coalesce(env("GIT_REPO"), "https://github.com/grafana/agent-modules.git")
    }

    argument "git_rev" {
      optional = true
      default = coalesce(env("GIT_REV"), env("GIT_REVISION"), env("GIT_BRANCH"), "main")
    }

    argument "git_pull_freq" {
      optional = true
      default = "0s"
    }

    module.git "endpoints_targets" {
      repository = argument.git_repo.value
      revision = argument.git_rev.value
      pull_frequency = argument.git_pull_freq.value
      path = "modules/kubernetes/metrics/targets/endpoints.river"

      arguments {
        tenant = argument.tenant.value
        git_repo = argument.git_repo.value
        git_rev = argument.git_rev.value
        git_pull_freq = argument.git_pull_freq.value
        scrape_port_named_metrics = argument.scrape_port_named_metrics.value
      }
    }

    prometheus.scrape "scrape_endpoints" {
      targets = module.git.endpoints_targets.exports.relabelings.output
      forward_to = [module.git.relabelings_kube_state_metrics.exports.metric_relabelings.receiver]

      clustering {
        enabled = argument.clustering.value
      }
    }

    // metric relabelings
    module.git "relabelings_kube_state_metrics" {
      repository = argument.git_repo.value
      revision = argument.git_rev.value
      pull_frequency = argument.git_pull_freq.value
      path = "modules/kubernetes/metrics/relabelings/kube-state-metrics.river"

      arguments {
        forward_to = [module.git.relabelings_node_exporter.exports.metric_relabelings.receiver]
      }
    }

    module.git "relabelings_node_exporter" {
      repository = argument.git_repo.value
      revision = argument.git_rev.value
      pull_frequency = argument.git_pull_freq.value
      path = "modules/kubernetes/metrics/relabelings/node-exporter.river"

      arguments {
        forward_to = [module.git.relabelings_opencost.exports.metric_relabelings.receiver]
      }
    }

    module.git "relabelings_opencost" {
      repository = argument.git_repo.value
      revision = argument.git_rev.value
      pull_frequency = argument.git_pull_freq.value
      path = "modules/kubernetes/metrics/relabelings/opencost.river"

      arguments {
        forward_to = [module.git.relabelings_auto_scrape.exports.metric_relabelings.receiver]
      }
    }

    // metric relabelings
    module.git "relabelings_auto_scrape" {
      repository = argument.git_repo.value
      revision = argument.git_rev.value
      pull_frequency = argument.git_pull_freq.value
      path = "modules/kubernetes/metrics/relabelings/auto-scrape.river"

      arguments {
        forward_to = argument.forward_to.value
        job_label = "kubernetes-endpoint-auto-scrape"
      }
    }
  auto-scrape-pods.river: |
    /*
    Module: scrape-pods
    Description:
      Kubernetes Pods Auto-Scraping
      -------------------------------------------------------------------------------------------------------
      !!! IMPORTANT !!!
      The annotations described below for auto-scraping of metrics should NOT be added to the both
      Service/Endpoints and to the Pods/Controller.  Metric scraping should be done at the endpoint level
      if at all possible.  The following annotations should only be added to a pod if the pod is not
      associated to a service
      !!! IMPORTANT !!!

      Each port attached to a pod container is an eligible target, oftentimes pods will have multiple ports.
      There may be instances when you want to scrape all ports or some ports and not others. To support this
      the following annotations are available:

      only scrape pods with scrape set to true, this can be single valued i.e. scrape all ports for
      the endpoint:

      metrics.agent.grafana.com/scrape: true
      prometheus.io/scrape: true

      the default scraping scheme is http, this can be specified as a single value which would override,
      the schema being used for all ports attached to the endpoint:

      metrics.agent.grafana.com/scheme: https
      prometheus.io/scheme: https

      the default path to scrape is /metrics, this can be specified as a single value which would override,
      the scrape path being used for all ports attached to the endpoint:

      metrics.agent.grafana.com/path: /metrics/some_path
      prometheus.io/path: /metrics/some_path

      the default port to scrape is the endpoint port, this can be specified as a single value which would
      override the scrape port being used for all ports attached to the endpoint, note that even if aan endpoint had
      multiple targets, the relabel_config targets are deduped before scraping:

      metrics.agent.grafana.com/port: 8080
      prometheus.io/port: 8080

      the default interval to scrape is 1m, this can be specified as a single value which would override,
      the scrape interval being used for all ports attached to the endpoint:

      metrics.agent.grafana.com/interval: 5m
      prometheus.io/interval: 5m

      the default timeout for scraping is 10s, this can be specified as a single value which would override,
      the scrape interval being used for all ports attached to the endpoint:

      metrics.agent.grafana.com/timeout: 30s
      prometheus.io/timeout: 30s
    */
    argument "forward_to" {
      // comment = "Must be a list(MetricssReceiver) where collected logs should be forwarded to"
      optional = false
    }

    argument "tenant" {
      // comment = "The tenant to filter logs to.  This does not have to be the tenantId, this is the value to look for in the logs.agent.grafana.com/tenant annotation, and this can be a regex."
      optional = true
      default = ".*"
    }

    argument "clustering" {
      // comment = "Whether or not clustering should be enabled"
      optional = true
      default = false
    }

    argument "git_repo" {
      optional = true
      default = coalesce(env("GIT_REPO"), "https://github.com/grafana/agent-modules.git")
    }

    argument "git_rev" {
      optional = true
      default = coalesce(env("GIT_REV"), env("GIT_REVISION"), env("GIT_BRANCH"), "main")
    }

    argument "git_pull_freq" {
      optional = true
      default = "0s"
    }

    module.git "pod_targets" {
      repository = argument.git_repo.value
      revision = argument.git_rev.value
      pull_frequency = argument.git_pull_freq.value
      path = "modules/kubernetes/metrics/targets/pods.river"

      arguments {
        tenant = argument.tenant.value
        git_repo = argument.git_repo.value
        git_rev = argument.git_rev.value
        git_pull_freq = argument.git_pull_freq.value
      }
    }

    prometheus.scrape "scrape_pods" {
      targets = module.git.pod_targets.exports.relabelings.output
      forward_to = [module.git.relabelings_auto_scrape.exports.metric_relabelings.receiver]

      clustering {
        enabled = argument.clustering.value
      }
    }

    // metric relabelings
    module.git "relabelings_auto_scrape" {
      repository = argument.git_repo.value
      revision = argument.git_rev.value
      pull_frequency = argument.git_pull_freq.value
      path = "modules/kubernetes/metrics/relabelings/auto-scrape.river"

      arguments {
        forward_to = argument.forward_to.value
        job_label = "kubernetes-pod-auto-scrape"
      }
    }
  grafana-cloud.river: "\n/********************************************\n * ARGUMENTS\n
    ********************************************/\nargument \"stack_name\" { }\n\nargument
    \"token\" { }\n\n/********************************************\n * EXPORTS\n ********************************************/\n\nexport
    \"metrics_receiver\" {\n\tvalue = prometheus.remote_write.default.receiver\n}\n\nexport
    \"logs_receiver\" {\n\tvalue = loki.write.default.receiver\n}\n\nexport \"traces_receiver\"
    {\n\tvalue = otelcol.exporter.otlp.default.input\n}\n\nexport \"profiles_receiver\"
    {\n\tvalue = pyroscope.write.default.receiver\n}\n\nexport \"stack_information\"
    {\n\tvalue = json_decode(remote.http.config_file.content)\n}\n\n/********************************************\n
    * External information\n ********************************************/\n\nremote.http
    \"config_file\" {\n\turl = \"https://grafana.com/api/instances/\" + argument.stack_name.value\n\n\tclient
    {\n\t\tbearer_token = argument.token.value\n\t}\n\tpoll_frequency = \"24h\"\n}\n\n/********************************************\n
    * Endpoints\n ********************************************/\n\n// Metrics\nprometheus.remote_write
    \"default\" {\n\tendpoint {\n\t\turl = json_decode(remote.http.config_file.content)[\"hmInstancePromUrl\"]
    + \"/api/prom/push\"\n\n\t\tbasic_auth {\n\t\t\tusername = json_decode(remote.http.config_file.content)[\"hmInstancePromId\"]\n\t\t\tpassword
    = argument.token.value\n\t\t}\n\t}\n}\n\n// Logs\nloki.write \"default\" {\n\tendpoint
    {\n\t\turl = json_decode(remote.http.config_file.content)[\"hlInstanceUrl\"] +
    \"/loki/api/v1/push\"\n\n\t\tbasic_auth {\n\t\t\tusername = json_decode(remote.http.config_file.content)[\"hlInstanceId\"]\n\t\t\tpassword
    = argument.token.value\n\t\t}\n\t}\n}\n\n// Traces\notelcol.auth.basic \"default\"
    {\n\tusername = json_decode(remote.http.config_file.content)[\"htInstanceId\"]\n\tpassword
    = argument.token.value\n}\n\notelcol.exporter.otlp \"default\" {\n\tclient {\n\t\tendpoint
    = json_decode(remote.http.config_file.content)[\"htInstanceUrl\"] + \":443\"\n\t\tauth
    \    = otelcol.auth.basic.default.handler\n\t}\n}\n\n// Profiles\npyroscope.write
    \"default\" {\n\tendpoint {\n\t\turl = json_decode(remote.http.config_file.content)[\"hpInstanceUrl\"]\n\n\t\tbasic_auth
    {\n\t\t\tusername = json_decode(remote.http.config_file.content)[\"hpInstanceId\"]\n\t\t\tpassword
    = argument.token.value\n\t\t}\n\t}\n}\n"
  kube-apiserver.river: |-
    /*
    Module: scrape-kube-apiserver
    Description: Scrapes Kube apiserver, most of these same metrics can come from cAdvisor use only if necessary
    */
    argument "forward_to" {
      // comment = "Must be a list(MetricssReceiver) where collected logs should be forwarded to"
      optional = false
    }

    argument "clustering" {
      // comment = "Whether or not clustering should be enabled"
      optional = true
      default = false
    }

    argument "job_label" {
      optional = true
      default = "integrations/kubernetes/apiserver"
      // comment = "The job label to add for all apiserver"
    }

    // drop metrics and les from kube-prometheus
    // https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/kubernetesControlPlane-serviceMonitorApiserver.yaml
    argument "drop_metrics" {
      optional = true
      default = "kubelet_(pod_worker_latency_microseconds|pod_start_latency_microseconds|cgroup_manager_latency_microseconds|pod_worker_start_latency_microseconds|pleg_relist_latency_microseconds|pleg_relist_interval_microseconds|runtime_operations|runtime_operations_latency_microseconds|runtime_operations_errors|eviction_stats_age_microseconds|device_plugin_registration_count|device_plugin_alloc_latency_microseconds|network_plugin_operations_latency_microseconds)|scheduler_(e2e_scheduling_latency_microseconds|scheduling_algorithm_predicate_evaluation|scheduling_algorithm_priority_evaluation|scheduling_algorithm_preemption_evaluation|scheduling_algorithm_latency_microseconds|binding_latency_microseconds|scheduling_latency_seconds)|apiserver_(request_count|request_latencies|request_latencies_summary|dropped_requests|storage_data_key_generation_latencies_microseconds|storage_transformation_failures_total|storage_transformation_latencies_microseconds|proxy_tunnel_sync_latency_secs|longrunning_gauge|registered_watchers)|kubelet_docker_(operations|operations_latency_microseconds|operations_errors|operations_timeout)|reflector_(items_per_list|items_per_watch|list_duration_seconds|lists_total|short_watches_total|watch_duration_seconds|watches_total)|etcd_(helper_cache_hit_count|helper_cache_miss_count|helper_cache_entry_count|object_counts|request_cache_get_latencies_summary|request_cache_add_latencies_summary|request_latencies_summary)|transformation_(transformation_latencies_microseconds|failures_total)|(admission_quota_controller_adds|admission_quota_controller_depth|admission_quota_controller_longest_running_processor_microseconds|admission_quota_controller_queue_latency|admission_quota_controller_unfinished_work_seconds|admission_quota_controller_work_duration|APIServiceOpenAPIAggregationControllerQueue1_adds|APIServiceOpenAPIAggregationControllerQueue1_depth|APIServiceOpenAPIAggregationControllerQueue1_longest_running_processor_microseconds|APIServiceOpenAPIAggregationControllerQueue1_queue_latency|APIServiceOpenAPIAggregationControllerQueue1_retries|APIServiceOpenAPIAggregationControllerQueue1_unfinished_work_seconds|APIServiceOpenAPIAggregationControllerQueue1_work_duration|APIServiceRegistrationController_adds|APIServiceRegistrationController_depth|APIServiceRegistrationController_longest_running_processor_microseconds|APIServiceRegistrationController_queue_latency|APIServiceRegistrationController_retries|APIServiceRegistrationController_unfinished_work_seconds|APIServiceRegistrationController_work_duration|autoregister_adds|autoregister_depth|autoregister_longest_running_processor_microseconds|autoregister_queue_latency|autoregister_retries|autoregister_unfinished_work_seconds|autoregister_work_duration|AvailableConditionController_adds|AvailableConditionController_depth|AvailableConditionController_longest_running_processor_microseconds|AvailableConditionController_queue_latency|AvailableConditionController_retries|AvailableConditionController_unfinished_work_seconds|AvailableConditionController_work_duration|crd_autoregistration_controller_adds|crd_autoregistration_controller_depth|crd_autoregistration_controller_longest_running_processor_microseconds|crd_autoregistration_controller_queue_latency|crd_autoregistration_controller_retries|crd_autoregistration_controller_unfinished_work_seconds|crd_autoregistration_controller_work_duration|crdEstablishing_adds|crdEstablishing_depth|crdEstablishing_longest_running_processor_microseconds|crdEstablishing_queue_latency|crdEstablishing_retries|crdEstablishing_unfinished_work_seconds|crdEstablishing_work_duration|crd_finalizer_adds|crd_finalizer_depth|crd_finalizer_longest_running_processor_microseconds|crd_finalizer_queue_latency|crd_finalizer_retries|crd_finalizer_unfinished_work_seconds|crd_finalizer_work_duration|crd_naming_condition_controller_adds|crd_naming_condition_controller_depth|crd_naming_condition_controller_longest_running_processor_microseconds|crd_naming_condition_controller_queue_latency|crd_naming_condition_controller_retries|crd_naming_condition_controller_unfinished_work_seconds|crd_naming_condition_controller_work_duration|crd_openapi_controller_adds|crd_openapi_controller_depth|crd_openapi_controller_longest_running_processor_microseconds|crd_openapi_controller_queue_latency|crd_openapi_controller_retries|crd_openapi_controller_unfinished_work_seconds|crd_openapi_controller_work_duration|DiscoveryController_adds|DiscoveryController_depth|DiscoveryController_longest_running_processor_microseconds|DiscoveryController_queue_latency|DiscoveryController_retries|DiscoveryController_unfinished_work_seconds|DiscoveryController_work_duration|kubeproxy_sync_proxy_rules_latency_microseconds|non_structural_schema_condition_controller_adds|non_structural_schema_condition_controller_depth|non_structural_schema_condition_controller_longest_running_processor_microseconds|non_structural_schema_condition_controller_queue_latency|non_structural_schema_condition_controller_retries|non_structural_schema_condition_controller_unfinished_work_seconds|non_structural_schema_condition_controller_work_duration|rest_client_request_latency_seconds|storage_operation_errors_total|storage_operation_status_count)|etcd_(debugging|disk|server).*|apiserver_admission_controller_admission_latencies_seconds_.*|apiserver_admission_step_admission_latencies_seconds_.*"
      // comment = "Regex of metrics to drop"
    }

    argument "drop_les" {
      optional = true
      default = "apiserver_request_duration_seconds_bucket;(0.15|0.25|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2.5|3|3.5|4.5|6|7|8|9|15|25|30|50)"
      // comment = "Regex of metric les to drop"
    }

    // get the available endpoints
    discovery.kubernetes "endpoints" {
      role = "endpoints"
    }

    /********************************************
     * Discovery the targets
     ********************************************/
    discovery.relabel "kube_apiserver" {
      targets = discovery.kubernetes.endpoints.targets

      // only keep namespace=default, service=kubernetes, port=https
      rule {
        action = "keep"
        source_labels = [
          "__meta_kubernetes_namespace",
          "__meta_kubernetes_service_name",
          "__meta_kubernetes_endpoint_port_name",
        ]
        regex = "default;kubernetes;https"
      }

      // set the namespace
      rule {
        action = "replace"
        source_labels = ["__meta_kubernetes_namespace"]
        target_label = "namespace"
      }

      // set the service_name
      rule {
        action = "replace"
        source_labels = ["__meta_kubernetes_service_name"]
        target_label = "service"
      }

    }

    /********************************************
     * Scrape metric
     ********************************************/
    prometheus.scrape "kube_apiserver" {
      targets = discovery.relabel.kube_apiserver.output
      scheme = "https"
      bearer_token_file = "/var/run/secrets/kubernetes.io/serviceaccount/token"
      forward_to = [prometheus.relabel.kube_apiserver.receiver]

      tls_config {
        ca_file = "/var/run/secrets/kubernetes.io/serviceaccount/ca.crt"
        insecure_skip_verify = false
        server_name = "kubernetes"
      }

      clustering {
        enabled = argument.clustering.value
      }
    }

    /********************************************
     * Metric relabelings
     ********************************************/
    prometheus.relabel "kube_apiserver" {
      forward_to = argument.forward_to.value

      // drop metrics
      rule {
        action = "drop"
        source_labels = ["__name__"]
        regex = argument.drop_metrics.value
      }

      // drop les
      rule {
        action = "drop"
        source_labels = [
          "__name__",
          "le",
        ]
        regex = argument.drop_les.value
      }

      // set the job label, only if job label is not set or contains "module." is not specified
      rule {
        action = "replace"
        source_labels = ["job"]
        regex = "|.*module\\..*"
        replacement = argument.job_label.value
        target_label = "job"
      }

    }
  lgtmp.river: "/********************************************\n * ARGUMENTS\n ********************************************/\nargument
    \"cluster\" {\n\toptional = true\n\tdefault  = \"monitoring-system\"\n}\n\nargument
    \"metrics_endpoint\" {\n\toptional = true\n\tdefault  = \"http://mimir:8080\"\n\t//comment
    = \"Where to send collected metrics.\"\n}\n\nargument \"logs_endpoint\" {\n\toptional
    = true\n\tdefault  = \"http://loki:3100\"\n\t//comment = \"Where to send collected
    logs.\"\n}\n\nargument \"traces_endpoint\" {\n\toptional = true\n\tdefault  =
    \"tempo:4317\"\n\t//comment = \"Where to send collected traces.\"\n}\n\nargument
    \"profiles_endpoint\" {\n\toptional = true\n\tdefault  = \"http://pyroscope:4040\"\n\t//comment
    \ = \"Where to send collected profiles.\"\n}\n\n/********************************************\n
    * EXPORTS\n ********************************************/\n\nexport \"metrics_receiver\"
    {\n\tvalue = prometheus.remote_write.mimir.receiver\n}\n\nexport \"logs_receiver\"
    {\n\tvalue = loki.write.loki.receiver\n}\n\nexport \"traces_receiver\" {\n\tvalue
    = otelcol.exporter.otlp.tempo.input\n}\n\nexport \"profiles_receiver\" {\n\tvalue
    = pyroscope.write.pyroscope.receiver\n}\n\n/********************************************\n
    * Endpoints\n ********************************************/\n\n// Metrics\nprometheus.remote_write
    \"mimir\" {\n\tendpoint {\n\t\turl = argument.metrics_endpoint.value + \"/api/v1/push\"\n\t}\n\n\texternal_labels
    = {\n\t\t\"scraped_by\" = \"grafana-agent\",\n\t\t\"cluster\" \t = argument.cluster.value,\n\t}\n}\n\n//
    Logs\nloki.write \"loki\" {\n\tendpoint {\n\t\turl = argument.logs_endpoint.value
    + \"/loki/api/v1/push\"\n\t}\n\n\texternal_labels = {\n\t\t\"scraped_by\" = \"grafana-agent\",\n\t\t\"cluster\"
    \t = argument.cluster.value,\n\t}\n}\n\n// Traces\notelcol.exporter.otlp \"tempo\"
    {\n\tclient {\n\t\tendpoint = argument.traces_endpoint.value\n\n\t\ttls {\n\t\t\tinsecure
    \            = true\n\t\t\tinsecure_skip_verify = true\n\t\t}\n\t}\n}\n\n// Profiles\npyroscope.write
    \"pyroscope\" {\n\tendpoint {\n\t\turl = argument.profiles_endpoint.value\n\t}\n\n\texternal_labels
    = {\n\t\t\"scraped_by\" = \"grafana-agent\",\n\t\t\"cluster\" \t = argument.cluster.value,\n\t}\n}\n"
  metrics-all.river: "/*\nModule: metrics-all\nDescription: Wrapper module to include
    all kubernetes metric modules and use cri parsing\n*/\nargument \"forward_to\"
    {\n  // comment = \"Must be a list(MetricssReceiver) where collected logs should
    be forwarded to\"\n\toptional = false\n}\n\nargument \"agent_config_folder\" {\n
    \ // comment = \"Whether or not clustering should be enabled\"\n  optional = true\n
    \ default = coalesce(env(\"AGENT_CONFIG_FOLDER\"), \"/etc/agent\")\n}\n\nargument
    \"scrape_port_named_metrics\" {\n  // comment = \"Whether or not to automatically
    scrape endpoints that have a port with 'metrics' in the name\"\n  optional = true\n
    \ default = false\n}\n\nargument \"clustering\" {\n  // comment = \"Whether or
    not clustering should be enabled\"\n  optional = true\n  default = false\n}\n\nargument
    \"tenant\" {\n  // comment = \"The tenant to filter logs to.  This does not have
    to be the tenantId, this is the value to look for in the logs.agent.grafana.com/tenant
    annotation, and this can be a regex.\"\n  optional = true\n  default = \".*\"\n}\n\nargument
    \"git_repo\" {\n  optional = true\n  default = coalesce(env(\"GIT_REPO\"), \"https://github.com/grafana/agent-modules.git\")\n}\n\nargument
    \"git_rev\" {\n  optional = true\n  default = coalesce(env(\"GIT_REV\"), env(\"GIT_REVISION\"),
    env(\"GIT_BRANCH\"), \"main\")\n}\n\nargument \"git_pull_freq\" {\n  optional
    = true\n  default = \"0s\"\n}\n\n\n/********************************************\n
    * Scrape Kube API Server\n ********************************************/\nmodule.file
    \"scrape_kube_apiserver\" {\n\tfilename = argument.agent_config_folder.value +
    \"/kube-apiserver.river\"\n\n  arguments {\n    forward_to = argument.forward_to.value\n
    \   clustering = argument.clustering.value\n  }\n}\n\n/********************************************\n
    * Scrape Kubelet\n ********************************************/\nmodule.git \"scrape_kubelet_cadvisor\"
    {\n  repository = argument.git_repo.value\n  revision = argument.git_rev.value\n
    \ pull_frequency = argument.git_pull_freq.value\n  path = \"modules/kubernetes/metrics/scrapes/kubelet-cadvisor.river\"\n\n
    \ arguments {\n    forward_to = argument.forward_to.value\n    tenant = argument.tenant.value\n
    \   clustering = argument.clustering.value\n    git_repo = argument.git_repo.value\n
    \   git_rev = argument.git_rev.value\n    git_pull_freq = argument.git_pull_freq.value\n
    \ }\n}\n\nmodule.git \"scrape_kubelet\" {\n  repository = argument.git_repo.value\n
    \ revision = argument.git_rev.value\n  pull_frequency = argument.git_pull_freq.value\n
    \ path = \"modules/kubernetes/metrics/scrapes/kubelet.river\"\n\n  arguments {\n
    \   forward_to = argument.forward_to.value\n    tenant = argument.tenant.value\n
    \   clustering = argument.clustering.value\n    git_repo = argument.git_repo.value\n
    \   git_rev = argument.git_rev.value\n    git_pull_freq = argument.git_pull_freq.value\n
    \ }\n}\n\nmodule.git \"scrape_kubelet_probes\" {\n  repository = argument.git_repo.value\n
    \ revision = argument.git_rev.value\n  pull_frequency = argument.git_pull_freq.value\n
    \ path = \"modules/kubernetes/metrics/scrapes/kubelet-probes.river\"\n\n  arguments
    {\n    forward_to = argument.forward_to.value\n    tenant = argument.tenant.value\n
    \   clustering = argument.clustering.value\n    git_repo = argument.git_repo.value\n
    \   git_rev = argument.git_rev.value\n    git_pull_freq = argument.git_pull_freq.value\n
    \ }\n}\n\n/********************************************\n * Kubernetes Auto Scrape
    Endpoints\n ********************************************/\nmodule.file \"auto_scrape_endpoints\"
    {\n\tfilename = argument.agent_config_folder.value + \"/auto-scrape-endpoints.river\"\n\n
    \ arguments {\n    forward_to = argument.forward_to.value\n    tenant = argument.tenant.value\n
    \   clustering = argument.clustering.value\n    scrape_port_named_metrics = argument.scrape_port_named_metrics.value\n
    \ }\n}\n\n/********************************************\n * Kubernetes Auto Scrape
    Pods\n ********************************************/\nmodule.file \"auto_scrape_pods\"
    {\n\tfilename = argument.agent_config_folder.value + \"/auto-scrape-pods.river\"\n\n
    \ arguments {\n    forward_to = argument.forward_to.value\n    tenant = argument.tenant.value\n
    \   clustering = argument.clustering.value\n  }\n}\n"
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: agent-modules-24tb9t4dfh
  namespace: monitoring-system
---
apiVersion: v1
data:
  ALERT_MANAGER_HOST: alertmanager-headless.monitoring-system.svc.cluster.local
  COMPACTOR_HOST: compactor.monitoring-system.svc.cluster.local
  DISTRIBUTOR_HOST: distributor.monitoring-system.svc.cluster.local
  NGINX_ENVSUBST_OUTPUT_DIR: /etc/nginx
  QUERY_FRONTEND_HOST: query-frontend.monitoring-system.svc.cluster.local
  RULER_HOST: ruler.monitoring-system.svc.cluster.local
immutable: true
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: nginx-env-d58ddffg6h
  namespace: monitoring-system
---
apiVersion: v1
data:
  gateway_mimir.conf.template: "server {\n    listen 8080;\n    listen [::]:8080;\n\n
    \   location = / {\n      return 200 'OK';\n      auth_basic off;\n      access_log
    off;\n    }\n\n    proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;\n\n
    \   # Distributor endpoints\n    location /distributor {\n      proxy_pass      http://${DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n    location = /api/v1/push {\n      proxy_pass      http://${DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n    location /otlp/v1/metrics {\n      proxy_pass      http://${DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n\n    # Alertmanager endpoints\n    location /alertmanager {\n      proxy_pass
    \     http://${ALERT_MANAGER_HOST}:8080$request_uri;\n    }\n    location = /multitenant_alertmanager/status
    {\n      proxy_pass      http://${ALERT_MANAGER_HOST}:8080$request_uri;\n    }\n
    \   location = /api/v1/alerts {\n      proxy_pass      http://${ALERT_MANAGER_HOST}:8080$request_uri;\n
    \   }\n\n    # Ruler endpoints\n    location /prometheus/config/v1/rules {\n      proxy_pass
    \     http://${RULER_HOST}:8080$request_uri;\n    }\n    location /prometheus/api/v1/rules
    {\n      proxy_pass      http://${RULER_HOST}:8080$request_uri;\n    }\n    \n
    \   location /prometheus/api/v1/alerts {\n      proxy_pass      http://${RULER_HOST}:8080$request_uri;\n
    \   }\n    location = /ruler/ring {\n      proxy_pass      http://${RULER_HOST}:8080$request_uri;\n
    \   }\n\n    # Rest of /prometheus goes to the query frontend\n    location /prometheus
    {\n      proxy_pass      http://${QUERY_FRONTEND_HOST}:8080$request_uri;\n    }\n\n
    \   # Buildinfo endpoint can go to any component\n    location = /api/v1/status/buildinfo
    {\n      proxy_pass      http://${QUERY_FRONTEND_HOST}:8080$request_uri;\n    }\n\n
    \   # Compactor endpoint for uploading blocks\n    location /api/v1/upload/block/
    {\n      proxy_pass      http://${COMPACTOR_HOST}:8080$request_uri;\n    }\n}"
  nginx.conf.template: |-
    worker_processes  auto;
    error_log  /dev/stderr error;
    pid        /tmp/nginx.pid;
    worker_rlimit_nofile 8192;

    events {
      worker_connections  4096;  ## Default: 1024
    }

    http {
      client_body_temp_path /tmp/client_temp;
      proxy_temp_path       /tmp/proxy_temp_path;
      fastcgi_temp_path     /tmp/fastcgi_temp;
      uwsgi_temp_path       /tmp/uwsgi_temp;
      scgi_temp_path        /tmp/scgi_temp;

      client_max_body_size  4M;

      proxy_read_timeout    600; ## 10 minutes
      proxy_send_timeout    600;
      proxy_connect_timeout 600;

      proxy_http_version    1.1;

      default_type application/octet-stream;
      log_format   main '$remote_addr - $remote_user [$time_local]  $status '
            '"$request" $body_bytes_sent "$http_referer" '
            '"$http_user_agent" "$http_x_forwarded_for"';

      map $status $loggable {
        ~^[23]  0;
        default 1;
      }

      access_log   /dev/stderr  main if=$loggable;

      sendfile     on;
      tcp_nopush   on;

      resolver kube-dns.kube-system.svc.cluster.local;

      # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
      map $http_x_scope_orgid $ensured_x_scope_orgid {
        default $http_x_scope_orgid;
        "" "anonymous";
      }

      include /etc/nginx/gateway_*.conf;
    }
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: nginx-templates-h69cm5877t
  namespace: monitoring-system
---
apiVersion: v1
data:
  blackbox.yaml: |
    modules:
      http_2xx:
        http:
          follow_redirects: true
          preferred_ip_protocol: ip4
          valid_http_versions:
          - HTTP/1.1
          - HTTP/2.0
        prober: http
        timeout: 5s
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: prometheus-blackbox-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-blackbox-exporter
    app.kubernetes.io/version: v0.24.0
    helm.sh/chart: prometheus-blackbox-exporter-8.4.0
    team: team-infra
  name: prometheus-blackbox-exporter
  namespace: monitoring-system
---
apiVersion: v1
data:
  runtime.yaml: |-
    ingester_limits: # limits that each ingester replica enforces
      max_ingestion_rate: 20000
      max_series: 1500000
      max_tenants: 1000
      max_inflight_push_requests: 30000

    distributor_limits: # limits that each distributor replica enforces
      max_ingestion_rate: 20000
      max_inflight_push_requests: 30000
      max_inflight_push_requests_bytes: 50000000

    overrides:
      anonymous: # limits for anonymous that the whole cluster enforces
        # ingestion_tenant_shard_size: 9
        max_global_series_per_user: 1500000
        max_fetched_series_per_query: 100000
immutable: true
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: runtime-config-d6979bkdgd
  namespace: monitoring-system
---
apiVersion: v1
data:
  AGENT_CONFIG_FOLDER: L2V0Yy9hZ2VudC1tb2R1bGVz
  AGENT_LOG_LEVEL: aW5mbw==
  CLUSTER: azNkLWszcy1jb2RlbGFi
  METRICS_ENDPOINT: aHR0cDovL25naW54Lm1vbml0b3Jpbmctc3lzdGVtOjgwODA=
kind: Secret
metadata:
  labels:
    team: team-infra
  name: agent-metrics-env-c9685b4855
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  alertmanager-fallback-config.yaml: |
    cm91dGU6CiAgZ3JvdXBfd2FpdDogMHMKICByZWNlaXZlcjogZW1wdHktcmVjZWl2ZXIKCn
    JlY2VpdmVyczoKICAjIEluIHRoaXMgZXhhbXBsZSB3ZSdyZSBub3QgZ29pbmcgdG8gc2Vu
    ZCBhbnkgbm90aWZpY2F0aW9uIG91dCBvZiBBbGVydG1hbmFnZXIuCiAgLSBuYW1lOiAnZW
    1wdHktcmVjZWl2ZXInCg==
  mimir.yaml: |
    IyBEbyBub3QgdXNlIHRoaXMgY29uZmlndXJhdGlvbiBpbiBwcm9kdWN0aW9uLgojIEl0IG
    lzIGZvciBkZW1vbnN0cmF0aW9uIHB1cnBvc2VzIG9ubHkuCm11bHRpdGVuYW5jeV9lbmFi
    bGVkOiBmYWxzZQoKIyAtdXNhZ2Utc3RhdHMuZW5hYmxlZD1mYWxzZQp1c2FnZV9zdGF0cz
    oKICBlbmFibGVkOiBmYWxzZQoKc2VydmVyOgogIGh0dHBfbGlzdGVuX3BvcnQ6IDgwODAK
    ICBncnBjX2xpc3Rlbl9wb3J0OiA5MDk1CiAgbG9nX2xldmVsOiB3YXJuCgpjb21tb246Ci
    Agc3RvcmFnZToKICAgIGJhY2tlbmQ6IHMzCiAgICBzMzoKICAgICAgZW5kcG9pbnQ6ICAg
    ICAgICAgIG1pbmlvOjkwMDAKICAgICAgYWNjZXNzX2tleV9pZDogICAgIGFkbWluCiAgIC
    AgIHNlY3JldF9hY2Nlc3Nfa2V5OiBhZG1pbl9wYXNzd29yZAogICAgICBpbnNlY3VyZTog
    ICAgICAgICAgdHJ1ZQoKYWxlcnRtYW5hZ2VyOgogIGRhdGFfZGlyOiAvZGF0YS9hbGVydG
    1hbmFnZXIKICBlbmFibGVfYXBpOiB0cnVlCiAgZXh0ZXJuYWxfdXJsOiAvYWxlcnRtYW5h
    Z2VyCiAgZmFsbGJhY2tfY29uZmlnX2ZpbGU6IC9ldGMvbWltaXIvYWxlcnRtYW5hZ2VyLW
    ZhbGxiYWNrLWNvbmZpZy55YW1sCmFsZXJ0bWFuYWdlcl9zdG9yYWdlOgogIHMzOgogICAg
    YnVja2V0X25hbWU6IG1pbWlyLWFsZXJ0bWFuYWdlcgoKYmxvY2tzX3N0b3JhZ2U6CiAgcz
    M6CiAgICBidWNrZXRfbmFtZTogbWltaXItZGF0YQogIHRzZGI6CiAgICBkaXI6IC9kYXRh
    L2luZ2VzdGVyCgpmcm9udGVuZDoKICBwYXJhbGxlbGl6ZV9zaGFyZGFibGVfcXVlcmllcz
    ogdHJ1ZQogIHNjaGVkdWxlcl9hZGRyZXNzOiBxdWVyeS1zY2hlZHVsZXItaGVhZGxlc3M6
    OTA5NQpmcm9udGVuZF93b3JrZXI6CiAgZ3JwY19jbGllbnRfY29uZmlnOgogICAgbWF4X3
    NlbmRfbXNnX3NpemU6IDQxOTQzMDQwMAogIHNjaGVkdWxlcl9hZGRyZXNzOiBxdWVyeS1z
    Y2hlZHVsZXItaGVhZGxlc3M6OTA5NQoKbWVtYmVybGlzdDoKICBqb2luX21lbWJlcnM6IF
    sgZ29zc2lwLXJpbmctaGVhZGxlc3M6Nzk0NiBdCgpydWxlcjoKICBydWxlX3BhdGg6IC9k
    YXRhL3J1bGVzCiAgZW5hYmxlX2FwaTogdHJ1ZQogIGFsZXJ0bWFuYWdlcl91cmw6IGh0dH
    A6Ly9hbGVydG1hbmFnZXItaGVhZGxlc3M6ODA4MC9hbGVydG1hbmFnZXIKcnVsZXJfc3Rv
    cmFnZToKICBzMzoKICAgIGJ1Y2tldF9uYW1lOiBtaW1pci1ydWxlcwoKcnVudGltZV9jb2
    5maWc6CiAgZmlsZTogL3Zhci9taW1pci9ydW50aW1lLnlhbWwKCnF1ZXJ5X3NjaGVkdWxl
    cjoKICBtYXhfb3V0c3RhbmRpbmdfcmVxdWVzdHNfcGVyX3RlbmFudDogODAwCgpsaW1pdH
    M6CiAgbmF0aXZlX2hpc3RvZ3JhbXNfaW5nZXN0aW9uX2VuYWJsZWQ6IHRydWUK
kind: Secret
metadata:
  labels:
    team: team-infra
  name: mimir-config-6f5cgttm66
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: alertmanager
    team: team-infra
  name: alertmanager
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-am
    port: 9095
  selector:
    app: alertmanager
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: alertmanager
    team: team-infra
  name: alertmanager-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: http-web
    port: 9093
  - name: tcp-cluster
    port: 9094
  - name: udp-cluster
    port: 9094
    protocol: UDP
  - name: grpc-am
    port: 9095
  publishNotReadyAddresses: true
  selector:
    app: alertmanager
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: compactor
    team: team-infra
  name: compactor
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-compactor
    port: 9095
  selector:
    app: compactor
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: distributor
    team: team-infra
  name: distributor
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-distribut
    port: 9095
  selector:
    app: distributor
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gossip-ring
    team: team-infra
  name: gossip-ring-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: tcp-gossip-ring
    port: 7946
    protocol: TCP
    targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    gossip_ring_member: "true"
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/name: grafana-agent
    team: team-infra
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent-cluster
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/name: grafana-agent
    team: team-infra
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingester
    team: team-infra
  name: ingester
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-ingester
    port: 9095
  selector:
    app: ingester
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingester
    team: team-infra
  name: ingester-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-ingester
    port: 9095
  selector:
    app: ingester
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: minio
    team: team-infra
  name: minio
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 9000
  selector:
    app: minio
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx
    team: team-infra
  name: nginx
  namespace: monitoring-system
spec:
  ports:
  - name: http-service
    port: 8080
  selector:
    app: nginx
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: overrides-exporter
    team: team-infra
  name: overrides-exporter
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-overrides
    port: 9095
  selector:
    app: overrides-exporter
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/interval: 15s
    prometheus.io/port: "9115"
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/instance: prometheus-blackbox-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-blackbox-exporter
    app.kubernetes.io/version: v0.24.0
    helm.sh/chart: prometheus-blackbox-exporter-8.4.0
    team: team-infra
  name: prometheus-blackbox-exporter
  namespace: monitoring-system
spec:
  ports:
  - name: http
    port: 9115
    protocol: TCP
    targetPort: http
  selector:
    app.kubernetes.io/instance: prometheus-blackbox-exporter
    app.kubernetes.io/name: prometheus-blackbox-exporter
    team: team-infra
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: "true"
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: prometheus-node-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/version: 1.6.1
    helm.sh/chart: prometheus-node-exporter-4.23.2
    team: team-infra
  name: prometheus-node-exporter
  namespace: monitoring-system
spec:
  ports:
  - name: metrics
    port: 9100
    protocol: TCP
    targetPort: 9100
  selector:
    app.kubernetes.io/instance: prometheus-node-exporter
    app.kubernetes.io/name: prometheus-node-exporter
    team: team-infra
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: querier
    team: team-infra
  name: querier
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-querier
    port: 9095
  selector:
    app: querier
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: query-frontend
    team: team-infra
  name: query-frontend
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-frontend
    port: 9095
  selector:
    app: query-frontend
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: query-scheduler
    team: team-infra
  name: query-scheduler
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-scheduler
    port: 9095
  selector:
    app: query-scheduler
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: query-scheduler
    team: team-infra
  name: query-scheduler-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-scheduler
    port: 9095
  publishNotReadyAddresses: true
  selector:
    app: query-scheduler
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ruler
    team: team-infra
  name: ruler
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  selector:
    app: ruler
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: store-gateway
    team: team-infra
  name: store-gateway
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-store-gw
    port: 9095
  selector:
    app: store-gateway
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: store-gateway
    team: team-infra
  name: store-gateway-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-store-gw
    port: 9095
  selector:
    app: store-gateway
    team: team-infra
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: distributor
    team: team-infra
  name: distributor
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: distributor
      team: team-infra
  template:
    metadata:
      labels:
        app: distributor
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=distributor
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: distributor
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-distribut
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 60
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: minio
    team: team-infra
  name: minio
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
      team: team-infra
  template:
    metadata:
      annotations:
        prometheus.io.path: /minio/v2/metrics/cluster
        prometheus.io.port: "9000"
        prometheus.io.scrape: "true"
      labels:
        app: minio
        team: team-infra
    spec:
      containers:
      - command:
        - sh
        - -c
        - |
          mkdir -p /data/mimir-data /data/mimir-rules /data/mimir-alertmanager && \
          mkdir -p /data/loki-data /data/loki-rules && \
          mkdir -p /data/tempo-data  && \
          mkdir -p /data/pyroscope-data && \
          minio server /data --console-address ':9001'
        env:
        - name: MINIO_PROMETHEUS_AUTH_TYPE
          value: public
        - name: MINIO_ROOT_USER
          value: admin
        - name: MINIO_ROOT_PASSWORD
          value: admin_password
        - name: MINIO_UPDATE
          value: "off"
        image: minio/minio:RELEASE.2023-07-21T21-12-44Z
        imagePullPolicy: IfNotPresent
        name: minio
        ports:
        - containerPort: 9000
          name: http-metrics
        - containerPort: 9001
          name: http-console
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
    team: team-infra
  name: nginx
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
      team: team-infra
  template:
    metadata:
      labels:
        app: nginx
        team: team-infra
    spec:
      containers:
      - envFrom:
        - configMapRef:
            name: nginx-env-d58ddffg6h
        image: nginxinc/nginx-unprivileged:1.25-alpine
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 8080
          name: http-service
        readinessProbe:
          httpGet:
            path: /
            port: http-service
          initialDelaySeconds: 15
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /etc/nginx/templates
          name: templates
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          name: nginx-templates-h69cm5877t
        name: templates
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: overrides-exporter
    team: team-infra
  name: overrides-exporter
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: overrides-exporter
      team: team-infra
  template:
    metadata:
      labels:
        app: overrides-exporter
        team: team-infra
    spec:
      containers:
      - args:
        - -target=overrides-exporter
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: overrides-exporter
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-overrides
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 60
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: prometheus-blackbox-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-blackbox-exporter
    app.kubernetes.io/version: v0.24.0
    helm.sh/chart: prometheus-blackbox-exporter-8.4.0
    team: team-infra
  name: prometheus-blackbox-exporter
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: prometheus-blackbox-exporter
      app.kubernetes.io/name: prometheus-blackbox-exporter
      team: team-infra
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 7af4981bbd5242641843c724fd5fd6b2de154a64e7fcfcddc55be1fb8099e6a9
      labels:
        app.kubernetes.io/instance: prometheus-blackbox-exporter
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: prometheus-blackbox-exporter
        app.kubernetes.io/version: v0.24.0
        helm.sh/chart: prometheus-blackbox-exporter-8.4.0
        team: team-infra
    spec:
      automountServiceAccountToken: false
      containers:
      - args:
        - --config.file=/config/blackbox.yaml
        env: null
        image: quay.io/prometheus/blackbox-exporter:v0.24.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /-/healthy
            port: http
        name: blackbox-exporter
        ports:
        - containerPort: 9115
          name: http
        readinessProbe:
          httpGet:
            path: /-/healthy
            port: http
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        volumeMounts:
        - mountPath: /config
          name: config
      hostNetwork: false
      restartPolicy: Always
      serviceAccountName: prometheus-blackbox-exporter
      volumes:
      - configMap:
          name: prometheus-blackbox-exporter
        name: config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: querier
    team: team-infra
  name: querier
  namespace: monitoring-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app: querier
      team: team-infra
  template:
    metadata:
      labels:
        app: querier
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=querier
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: querier
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-querier
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: query-frontend
    team: team-infra
  name: query-frontend
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: query-frontend
      team: team-infra
  template:
    metadata:
      labels:
        app: query-frontend
        team: team-infra
    spec:
      containers:
      - args:
        - -target=query-frontend
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: query-frontend
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-frontend
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: query-scheduler
    team: team-infra
  name: query-scheduler
  namespace: monitoring-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app: query-scheduler
      team: team-infra
  template:
    metadata:
      labels:
        app: query-scheduler
        team: team-infra
    spec:
      containers:
      - args:
        - -target=query-scheduler
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -server.grpc.keepalive.max-connection-age=2562047h
        - -server.grpc.keepalive.max-connection-age-grace=2562047h
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: query-scheduler
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-scheduler
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ruler
    team: team-infra
  name: ruler
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ruler
      team: team-infra
  template:
    metadata:
      labels:
        app: ruler
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=ruler
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: ruler
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-ruler
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /rules
          name: rule-path
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
      - emptyDir: {}
        name: rule-path
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: alertmanager
    team: team-infra
  name: alertmanager
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
      team: team-infra
  serviceName: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=alertmanager
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: alertmanager
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-am
        - containerPort: 9093
          name: http-web
        - containerPort: 9094
          name: tcp-cluster
        - containerPort: 9094
          name: ucp-cluster
          protocol: UDP
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 60
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: compactor
    team: team-infra
  name: compactor
  namespace: monitoring-system
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app: compactor
      team: team-infra
  serviceName: compactor
  template:
    metadata:
      labels:
        app: compactor
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=compactor
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: compactor
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-compactor
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 240
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
spec:
  minReadySeconds: 10
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: grafana-agent
      app.kubernetes.io/name: grafana-agent
      team: team-infra
  serviceName: grafana-agent
  template:
    metadata:
      annotations:
        metrics.agent.grafana.com/interval: 15s
        metrics.agent.grafana.com/scrape: "true"
      labels:
        app.kubernetes.io/instance: grafana-agent
        app.kubernetes.io/name: grafana-agent
        team: team-infra
    spec:
      containers:
      - args:
        - run
        - /etc/agent/metrics.river
        - --storage.path=/tmp/agent
        - --server.http.listen-addr=0.0.0.0:80
        - --server.http.ui-path-prefix=/
        - --disable-reporting
        - --cluster.enabled=true
        - --cluster.join-addresses=grafana-agent-cluster
        env:
        - name: AGENT_MODE
          value: flow
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        envFrom:
        - secretRef:
            name: agent-metrics-env-c9685b4855
        image: docker.io/grafana/agent:v0.37.4
        imagePullPolicy: IfNotPresent
        name: grafana-agent
        ports:
        - containerPort: 80
          name: http-metrics
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 80
          initialDelaySeconds: 10
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /etc/agent
          name: config
        - mountPath: /etc/agent-modules
          name: agent-modules
      - args:
        - --volume-dir=/etc/agent
        - --webhook-url=http://localhost:80/-/reload
        image: docker.io/jimmidyson/configmap-reload:v0.8.0
        name: config-reloader
        resources:
          requests:
            cpu: 1m
            memory: 5Mi
        volumeMounts:
        - mountPath: /etc/agent
          name: config
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: grafana-agent
      volumes:
      - configMap:
          name: agent-metrics-config-t5d9gg7fc6
        name: config
      - configMap:
          name: agent-modules-24tb9t4dfh
        name: agent-modules
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: ingester
    team: team-infra
  name: ingester
  namespace: monitoring-system
spec:
  podManagementPolicy: Parallel
  replicas: 2
  selector:
    matchLabels:
      app: ingester
      team: team-infra
  serviceName: ingester-headless
  template:
    metadata:
      labels:
        app: ingester
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=ingester
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -ingester.ring.instance-availability-zone=zone-default
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: ingester
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-ingester
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 240
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: store-gateway
    team: team-infra
  name: store-gateway
  namespace: monitoring-system
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app: store-gateway
      team: team-infra
  serviceName: store-gateway-headless
  template:
    metadata:
      labels:
        app: store-gateway
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=store-gateway
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: store-gateway
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc-store-gw
          protocol: TCP
        - containerPort: 7946
          name: http-memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 240
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app.kubernetes.io/component: metrics
    app.kubernetes.io/instance: prometheus-node-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-node-exporter
    app.kubernetes.io/part-of: prometheus-node-exporter
    app.kubernetes.io/version: 1.6.1
    helm.sh/chart: prometheus-node-exporter-4.23.2
    team: team-infra
  name: prometheus-node-exporter
  namespace: monitoring-system
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: prometheus-node-exporter
      app.kubernetes.io/name: prometheus-node-exporter
      team: team-infra
  template:
    metadata:
      annotations:
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"
      labels:
        app.kubernetes.io/component: metrics
        app.kubernetes.io/instance: prometheus-node-exporter
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: prometheus-node-exporter
        app.kubernetes.io/part-of: prometheus-node-exporter
        app.kubernetes.io/version: 1.6.1
        helm.sh/chart: prometheus-node-exporter-4.23.2
        team: team-infra
    spec:
      automountServiceAccountToken: false
      containers:
      - args:
        - --path.procfs=/host/proc
        - --path.sysfs=/host/sys
        - --path.rootfs=/host/root
        - --path.udev.data=/host/root/run/udev/data
        - --web.listen-address=[$(HOST_IP)]:9100
        env:
        - name: HOST_IP
          value: 0.0.0.0
        image: quay.io/prometheus/node-exporter:v1.6.1
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders: null
            path: /
            port: 9100
            scheme: HTTP
          initialDelaySeconds: 0
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: node-exporter
        ports:
        - containerPort: 9100
          name: metrics
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders: null
            path: /
            port: 9100
            scheme: HTTP
          initialDelaySeconds: 0
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        securityContext:
          readOnlyRootFilesystem: true
        volumeMounts:
        - mountPath: /host/proc
          name: proc
          readOnly: true
        - mountPath: /host/sys
          name: sys
          readOnly: true
        - mountPath: /host/root
          mountPropagation: HostToContainer
          name: root
          readOnly: true
      hostNetwork: true
      hostPID: true
      nodeSelector:
        kubernetes.io/os: linux
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      serviceAccountName: prometheus-node-exporter
      tolerations:
      - effect: NoSchedule
        operator: Exists
      - effect: NoExecute
        operator: Exists
      volumes:
      - hostPath:
          path: /proc
        name: proc
      - hostPath:
          path: /sys
        name: sys
      - hostPath:
          path: /
        name: root
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
    type: RollingUpdate
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
spec:
  rules:
  - host: grafana-agent-metrics.localhost
    http:
      paths:
      - backend:
          service:
            name: grafana-agent
            port:
              number: 80
        path: /
        pathType: Prefix
