apiVersion: v1
kind: Namespace
metadata:
  labels:
    team: team-infra
  name: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 10.1.5
    helm.sh/chart: grafana-7.0.3
    team: team-infra
  name: grafana
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: prometheus-blackbox-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-blackbox-exporter
    app.kubernetes.io/version: v0.24.0
    helm.sh/chart: prometheus-blackbox-exporter-8.4.0
    team: team-infra
  name: prometheus-blackbox-exporter
  namespace: monitoring-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 10.1.5
    helm.sh/chart: grafana-7.0.3
    team: team-infra
  name: grafana
  namespace: monitoring-system
rules: []
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
rules:
- apiGroups:
  - ""
  - discovery.k8s.io
  - networking.k8s.io
  resources:
  - endpoints
  - endpointslices
  - ingresses
  - nodes
  - nodes/proxy
  - nodes/metrics
  - pods
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  - pods/log
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.grafana.com
  resources:
  - podlogs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheusrules
  verbs:
  - get
  - list
  - watch
- nonResourceURLs:
  - /metrics
  verbs:
  - get
- apiGroups:
  - monitoring.coreos.com
  resources:
  - podmonitors
  - servicemonitors
  - probes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 10.1.5
    helm.sh/chart: grafana-7.0.3
    team: team-infra
  name: grafana-clusterrole
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - get
  - watch
  - list
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 10.1.5
    helm.sh/chart: grafana-7.0.3
    team: team-infra
  name: grafana
  namespace: monitoring-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: grafana
subjects:
- kind: ServiceAccount
  name: grafana
  namespace: monitoring-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: grafana-agent
subjects:
- kind: ServiceAccount
  name: grafana-agent
  namespace: monitoring-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 10.1.5
    helm.sh/chart: grafana-7.0.3
    team: team-infra
  name: grafana-clusterrolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: grafana-clusterrole
subjects:
- kind: ServiceAccount
  name: grafana
  namespace: monitoring-system
---
apiVersion: v1
data:
  grafana.ini: |
    [log]
    level = warn
    [log.frontend]
    enabled = false
    provider = grafana

    [analytics]
    reporting_enabled = false
    check_for_updates = false
    check_for_plugin_updates = false

    [auth.anonymous]
    enabled = true
    org_role = Admin

    [security]
    disable_gravatar = true
    angular_support_enabled = false

    [explore]
    enabled = true

    [users]
    default_theme = dark

    [metrics]
    enabled = true
    disable_total_stats = false

    [date_formats]
    use_browser_locale = true
    date_format_use_browser_locale = true

    [server]
    enable_gzip = true

    [dashboards]
    default_home_dashboard_path = /dashboards/minio-dashboard.json
    ;default_home_dashboard_path = /var/lib/grafana/dashboards/minio-dashboard.json
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: grafana-8bg2h9g669
  namespace: monitoring-system
---
apiVersion: v1
data:
  metrics.river: "logging {\n  level  = \"info\"\n  format = \"logfmt\"\n}\n\nmodule.git
    \"metrics_primary\" {\n  repository = \"https://github.com/grafana/agent-modules.git\"\n
    \ revision   = \"main\"\n  path       = \"modules/kubernetes/metrics/all.river\"\n\n
    \ arguments {\n    forward_to   = [prometheus.remote_write.local_primary.receiver]\n
    \   clustering   = false\n    blackbox_url = \"prometheus-blackbox-exporter.monitoring-system.svc.cluster.local:9115\"\n
    \   git_pull_freq= \"60s\"\n  }\n}\n\nprometheus.remote_write \"local_primary\"
    {\n  endpoint {\n    url = \"http://nginx.monitoring-system:8080/api/v1/push\"\n
    \ }\n\n  external_labels = {\n    \"scraped_by\" = \"grafana-agent\",\n    \"cluster\"
    \t = \"k3d-k3s-codelab\",\n  }\n}"
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: grafana-agent-metrics-config-m75g754mc5
  namespace: monitoring-system
---
apiVersion: v1
data:
  provider.yaml: |-
    apiVersion: 1
    providers:
      - name: 'sidecarProvider'
        orgId: 1
        type: file
        disableDeletion: false
        allowUiUpdates: false
        updateIntervalSeconds: 30
        options:
          foldersFromFilesStructure: true
          path: /dashboards
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 10.1.5
    helm.sh/chart: grafana-7.0.3
    team: team-infra
  name: grafana-config-dashboards
  namespace: monitoring-system
---
apiVersion: v1
data:
  datasources.yaml: |
    apiVersion: 1

    datasources:
    # Mimir for metrics
    - name: Metrics
      type: prometheus
      uid: metrics
      access: proxy
      orgId: 1
      url: http://nginx:8080/prometheus
      basicAuth: false
      isDefault: true
      version: 1
      editable: true

    # Loki for logs
    - name: Logs
      type: loki
      access: proxy
      orgId: 1
      uid: logs
      url: http://nginx:3100
      basicAuth: false
      isDefault: false
      version: 1
      editable: true

    # Tempo for traces
    - name: Traces
      type: tempo
      access: proxy
      orgId: 1
      uid: traces
      url: http://nginx:3200
      basicAuth: false
      isDefault: false
      version: 1
      editable: true
      apiVersion: 1
kind: ConfigMap
metadata:
  labels:
    grafana_datasource: "1"
    team: team-infra
  name: grafana-datasources-522m52k9bm
  namespace: monitoring-system
---
apiVersion: v1
data:
  GF_LOG_LEVEL: error
  NAMESPACE: monitoring-system
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: grafana-env-dkmb477tfh
  namespace: monitoring-system
---
apiVersion: v1
data:
  ALERT_MANAGER_HOST: alertmanager-headless.monitoring-system.svc.cluster.local
  COMPACTOR_HOST: compactor.monitoring-system.svc.cluster.local
  DISTRIBUTOR_HOST: distributor.monitoring-system.svc.cluster.local
  NGINX_ENVSUBST_OUTPUT_DIR: /etc/nginx
  QUERY_FRONTEND_HOST: query-frontend.monitoring-system.svc.cluster.local
  RULER_HOST: ruler.monitoring-system.svc.cluster.local
immutable: true
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: nginx-env-d58ddffg6h
  namespace: monitoring-system
---
apiVersion: v1
data:
  gateway_mimir.conf.template: "server {\n    listen 8080;\n    listen [::]:8080;\n\n
    \   location = / {\n      return 200 'OK';\n      auth_basic off;\n      access_log
    off;\n    }\n\n    proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;\n\n
    \   # Distributor endpoints\n    location /distributor {\n      proxy_pass      http://${DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n    location = /api/v1/push {\n      proxy_pass      http://${DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n    location /otlp/v1/metrics {\n      proxy_pass      http://${DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n\n    # Alertmanager endpoints\n    location /alertmanager {\n      proxy_pass
    \     http://${ALERT_MANAGER_HOST}:8080$request_uri;\n    }\n    location = /multitenant_alertmanager/status
    {\n      proxy_pass      http://${ALERT_MANAGER_HOST}:8080$request_uri;\n    }\n
    \   location = /api/v1/alerts {\n      proxy_pass      http://${ALERT_MANAGER_HOST}:8080$request_uri;\n
    \   }\n\n    # Ruler endpoints\n    location /prometheus/config/v1/rules {\n      proxy_pass
    \     http://${RULER_HOST}:8080$request_uri;\n    }\n    location /prometheus/api/v1/rules
    {\n      proxy_pass      http://${RULER_HOST}:8080$request_uri;\n    }\n    \n
    \   location /prometheus/api/v1/alerts {\n      proxy_pass      http://${RULER_HOST}:8080$request_uri;\n
    \   }\n    location = /ruler/ring {\n      proxy_pass      http://${RULER_HOST}:8080$request_uri;\n
    \   }\n\n    # Rest of /prometheus goes to the query frontend\n    location /prometheus
    {\n      proxy_pass      http://${QUERY_FRONTEND_HOST}:8080$request_uri;\n    }\n\n
    \   # Buildinfo endpoint can go to any component\n    location = /api/v1/status/buildinfo
    {\n      proxy_pass      http://${QUERY_FRONTEND_HOST}:8080$request_uri;\n    }\n\n
    \   # Compactor endpoint for uploading blocks\n    location /api/v1/upload/block/
    {\n      proxy_pass      http://${COMPACTOR_HOST}:8080$request_uri;\n    }\n}"
  nginx.conf.template: |-
    worker_processes  auto;
    error_log  /dev/stderr error;
    pid        /tmp/nginx.pid;
    worker_rlimit_nofile 8192;

    events {
      worker_connections  4096;  ## Default: 1024
    }

    http {
      client_body_temp_path /tmp/client_temp;
      proxy_temp_path       /tmp/proxy_temp_path;
      fastcgi_temp_path     /tmp/fastcgi_temp;
      uwsgi_temp_path       /tmp/uwsgi_temp;
      scgi_temp_path        /tmp/scgi_temp;

      client_max_body_size  4M;

      proxy_read_timeout    600; ## 10 minutes
      proxy_send_timeout    600;
      proxy_connect_timeout 600;

      proxy_http_version    1.1;

      default_type application/octet-stream;
      log_format   main '$remote_addr - $remote_user [$time_local]  $status '
            '"$request" $body_bytes_sent "$http_referer" '
            '"$http_user_agent" "$http_x_forwarded_for"';

      map $status $loggable {
        ~^[23]  0;
        default 1;
      }

      access_log   /dev/stderr  main if=$loggable;

      sendfile     on;
      tcp_nopush   on;

      resolver kube-dns.kube-system.svc.cluster.local;

      # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
      map $http_x_scope_orgid $ensured_x_scope_orgid {
        default $http_x_scope_orgid;
        "" "anonymous";
      }

      include /etc/nginx/gateway_*.conf;
    }
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: nginx-templates-h69cm5877t
  namespace: monitoring-system
---
apiVersion: v1
data:
  blackbox.yaml: |
    modules:
      http_2xx:
        http:
          follow_redirects: true
          preferred_ip_protocol: ip4
          valid_http_versions:
          - HTTP/1.1
          - HTTP/2.0
        prober: http
        timeout: 5s
kind: ConfigMap
metadata:
  labels:
    app.kubernetes.io/instance: prometheus-blackbox-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-blackbox-exporter
    app.kubernetes.io/version: v0.24.0
    helm.sh/chart: prometheus-blackbox-exporter-8.4.0
    team: team-infra
  name: prometheus-blackbox-exporter
  namespace: monitoring-system
---
apiVersion: v1
data:
  runtime.yaml: |-
    ingester_limits: # limits that each ingester replica enforces
      max_ingestion_rate: 20000
      max_series: 1500000
      max_tenants: 1000
      max_inflight_push_requests: 30000

    distributor_limits: # limits that each distributor replica enforces
      max_ingestion_rate: 20000
      max_inflight_push_requests: 30000
      max_inflight_push_requests_bytes: 50000000

    overrides:
      anonymous: # limits for anonymous that the whole cluster enforces
        # ingestion_tenant_shard_size: 9
        max_global_series_per_user: 1500000
        max_fetched_series_per_query: 100000
immutable: true
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: runtime-config-d6979bkdgd
  namespace: monitoring-system
---
apiVersion: v1
data:
  admin-password: YWRtaW5fcGFzc3dvcmQ=
  admin-user: YWRtaW4=
kind: Secret
metadata:
  labels:
    team: team-infra
  name: grafana-secret-55dh9ff969
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  alertmanager-fallback-config.yaml: |
    cm91dGU6CiAgZ3JvdXBfd2FpdDogMHMKICByZWNlaXZlcjogZW1wdHktcmVjZWl2ZXIKCn
    JlY2VpdmVyczoKICAjIEluIHRoaXMgZXhhbXBsZSB3ZSdyZSBub3QgZ29pbmcgdG8gc2Vu
    ZCBhbnkgbm90aWZpY2F0aW9uIG91dCBvZiBBbGVydG1hbmFnZXIuCiAgLSBuYW1lOiAnZW
    1wdHktcmVjZWl2ZXInCg==
  mimir.yaml: |
    IyBEbyBub3QgdXNlIHRoaXMgY29uZmlndXJhdGlvbiBpbiBwcm9kdWN0aW9uLgojIEl0IG
    lzIGZvciBkZW1vbnN0cmF0aW9uIHB1cnBvc2VzIG9ubHkuCm11bHRpdGVuYW5jeV9lbmFi
    bGVkOiBmYWxzZQoKIyAtdXNhZ2Utc3RhdHMuZW5hYmxlZD1mYWxzZQp1c2FnZV9zdGF0cz
    oKICBlbmFibGVkOiBmYWxzZQoKc2VydmVyOgogIGh0dHBfbGlzdGVuX3BvcnQ6IDgwODAK
    ICBncnBjX2xpc3Rlbl9wb3J0OiA5MDk1CiAgbG9nX2xldmVsOiB3YXJuCgpjb21tb246Ci
    Agc3RvcmFnZToKICAgIGJhY2tlbmQ6IHMzCiAgICBzMzoKICAgICAgZW5kcG9pbnQ6ICAg
    ICAgICAgIG1pbmlvOjkwMDAKICAgICAgYWNjZXNzX2tleV9pZDogICAgIGFkbWluCiAgIC
    AgIHNlY3JldF9hY2Nlc3Nfa2V5OiBhZG1pbl9wYXNzd29yZAogICAgICBpbnNlY3VyZTog
    ICAgICAgICAgdHJ1ZQoKYWxlcnRtYW5hZ2VyOgogIGRhdGFfZGlyOiAvZGF0YS9hbGVydG
    1hbmFnZXIKICBlbmFibGVfYXBpOiB0cnVlCiAgZXh0ZXJuYWxfdXJsOiAvYWxlcnRtYW5h
    Z2VyCiAgZmFsbGJhY2tfY29uZmlnX2ZpbGU6IC9ldGMvbWltaXIvYWxlcnRtYW5hZ2VyLW
    ZhbGxiYWNrLWNvbmZpZy55YW1sCmFsZXJ0bWFuYWdlcl9zdG9yYWdlOgogIHMzOgogICAg
    YnVja2V0X25hbWU6IG1pbWlyLWFsZXJ0bWFuYWdlcgoKYmxvY2tzX3N0b3JhZ2U6CiAgcz
    M6CiAgICBidWNrZXRfbmFtZTogbWltaXItZGF0YQogIHRzZGI6CiAgICBkaXI6IC9kYXRh
    L2luZ2VzdGVyCgpmcm9udGVuZDoKICBwYXJhbGxlbGl6ZV9zaGFyZGFibGVfcXVlcmllcz
    ogdHJ1ZQogIHNjaGVkdWxlcl9hZGRyZXNzOiBxdWVyeS1zY2hlZHVsZXItaGVhZGxlc3M6
    OTA5NQpmcm9udGVuZF93b3JrZXI6CiAgZ3JwY19jbGllbnRfY29uZmlnOgogICAgbWF4X3
    NlbmRfbXNnX3NpemU6IDQxOTQzMDQwMAogIHNjaGVkdWxlcl9hZGRyZXNzOiBxdWVyeS1z
    Y2hlZHVsZXItaGVhZGxlc3M6OTA5NQoKbWVtYmVybGlzdDoKICBqb2luX21lbWJlcnM6IF
    sgZ29zc2lwLXJpbmctaGVhZGxlc3M6Nzk0NiBdCgpydWxlcjoKICBydWxlX3BhdGg6IC9k
    YXRhL3J1bGVzCiAgZW5hYmxlX2FwaTogdHJ1ZQogIGFsZXJ0bWFuYWdlcl91cmw6IGh0dH
    A6Ly9hbGVydG1hbmFnZXItaGVhZGxlc3M6ODA4MC9hbGVydG1hbmFnZXIKcnVsZXJfc3Rv
    cmFnZToKICBzMzoKICAgIGJ1Y2tldF9uYW1lOiBtaW1pci1ydWxlcwoKcnVudGltZV9jb2
    5maWc6CiAgZmlsZTogL3Zhci9taW1pci9ydW50aW1lLnlhbWwKCnF1ZXJ5X3NjaGVkdWxl
    cjoKICBtYXhfb3V0c3RhbmRpbmdfcmVxdWVzdHNfcGVyX3RlbmFudDogODAwCgpsaW1pdH
    M6CiAgbmF0aXZlX2hpc3RvZ3JhbXNfaW5nZXN0aW9uX2VuYWJsZWQ6IHRydWUK
kind: Secret
metadata:
  labels:
    team: team-infra
  name: mimir-config-6f5cgttm66
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: alertmanager
    team: team-infra
  name: alertmanager
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-am
    port: 9095
  selector:
    app: alertmanager
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: alertmanager
    team: team-infra
  name: alertmanager-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: http-web
    port: 9093
  - name: tcp-cluster
    port: 9094
  - name: udp-cluster
    port: 9094
    protocol: UDP
  - name: grpc-am
    port: 9095
  publishNotReadyAddresses: true
  selector:
    app: alertmanager
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: compactor
    team: team-infra
  name: compactor
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-compactor
    port: 9095
  selector:
    app: compactor
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: distributor
    team: team-infra
  name: distributor
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-distribut
    port: 9095
  selector:
    app: distributor
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gossip-ring
    team: team-infra
  name: gossip-ring-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: tcp-gossip-ring
    port: 7946
    protocol: TCP
    targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    gossip_ring_member: "true"
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 10.1.5
    helm.sh/chart: grafana-7.0.3
    team: team-infra
  name: grafana
  namespace: monitoring-system
spec:
  ports:
  - name: service
    port: 80
    protocol: TCP
    targetPort: 3000
  selector:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/name: grafana
    team: team-infra
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/name: grafana-agent
    team: team-infra
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent-cluster
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/name: grafana-agent
    team: team-infra
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingester
    team: team-infra
  name: ingester
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-ingester
    port: 9095
  selector:
    app: ingester
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingester
    team: team-infra
  name: ingester-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-ingester
    port: 9095
  selector:
    app: ingester
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: minio
    team: team-infra
  name: minio
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 9000
  selector:
    app: minio
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx
    team: team-infra
  name: nginx
  namespace: monitoring-system
spec:
  ports:
  - name: http-service
    port: 8080
  selector:
    app: nginx
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: overrides-exporter
    team: team-infra
  name: overrides-exporter
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-overrides
    port: 9095
  selector:
    app: overrides-exporter
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: prometheus-blackbox-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-blackbox-exporter
    app.kubernetes.io/version: v0.24.0
    helm.sh/chart: prometheus-blackbox-exporter-8.4.0
    team: team-infra
  name: prometheus-blackbox-exporter
  namespace: monitoring-system
spec:
  ports:
  - name: http
    port: 9115
    protocol: TCP
    targetPort: http
  selector:
    app.kubernetes.io/instance: prometheus-blackbox-exporter
    app.kubernetes.io/name: prometheus-blackbox-exporter
    team: team-infra
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: querier
    team: team-infra
  name: querier
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-querier
    port: 9095
  selector:
    app: querier
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: query-frontend
    team: team-infra
  name: query-frontend
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-frontend
    port: 9095
  selector:
    app: query-frontend
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: query-scheduler
    team: team-infra
  name: query-scheduler
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-scheduler
    port: 9095
  selector:
    app: query-scheduler
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: query-scheduler
    team: team-infra
  name: query-scheduler-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-scheduler
    port: 9095
  publishNotReadyAddresses: true
  selector:
    app: query-scheduler
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ruler
    team: team-infra
  name: ruler
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  selector:
    app: ruler
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: store-gateway
    team: team-infra
  name: store-gateway
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-store-gw
    port: 9095
  selector:
    app: store-gateway
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: store-gateway
    team: team-infra
  name: store-gateway-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-store-gw
    port: 9095
  selector:
    app: store-gateway
    team: team-infra
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: distributor
    team: team-infra
  name: distributor
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: distributor
      team: team-infra
  template:
    metadata:
      labels:
        app: distributor
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=distributor
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: distributor
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-distribut
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 60
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana
    app.kubernetes.io/version: 10.1.5
    helm.sh/chart: grafana-7.0.3
    team: team-infra
  name: grafana
  namespace: monitoring-system
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/instance: grafana
      app.kubernetes.io/name: grafana
      team: team-infra
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/sc-dashboard-provider-config: 70a9ff9964ce37b52f90694f618606d9098b0767172fcbc0902d716bd7098e45
        kubectl.kubernetes.io/default-container: grafana
        metrics.agent.grafana.com/interval: 15s
        metrics.agent.grafana.com/port: "3000"
        metrics.agent.grafana.com/scrape: "true"
      labels:
        app.kubernetes.io/instance: grafana
        app.kubernetes.io/name: grafana
        team: team-infra
    spec:
      automountServiceAccountToken: true
      containers:
      - env:
        - name: METHOD
          value: WATCH
        - name: LABEL
          value: grafana_dashboard
        - name: LABEL_VALUE
          value: "1"
        - name: FOLDER
          value: /dashboards
        - name: RESOURCE
          value: both
        - name: NAMESPACE
          value: monitoring-system
        - name: FOLDER_ANNOTATION
          value: grafana_dashboard_folder
        - name: REQ_USERNAME
          valueFrom:
            secretKeyRef:
              key: admin-user
              name: grafana-secret-55dh9ff969
        - name: REQ_PASSWORD
          valueFrom:
            secretKeyRef:
              key: admin-password
              name: grafana-secret-55dh9ff969
        - name: REQ_URL
          value: http://localhost:3000/api/admin/provisioning/dashboards/reload
        - name: REQ_METHOD
          value: POST
        image: quay.io/kiwigrid/k8s-sidecar:1.25.2
        imagePullPolicy: IfNotPresent
        name: grafana-sc-dashboard
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /dashboards
          name: sc-dashboard-volume
      - env:
        - name: METHOD
          value: WATCH
        - name: LABEL
          value: grafana_datasource
        - name: LABEL_VALUE
          value: "1"
        - name: FOLDER
          value: /etc/grafana/provisioning/datasources
        - name: RESOURCE
          value: both
        - name: NAMESPACE
          value: monitoring-system
        - name: REQ_USERNAME
          valueFrom:
            secretKeyRef:
              key: admin-user
              name: grafana-secret-55dh9ff969
        - name: REQ_PASSWORD
          valueFrom:
            secretKeyRef:
              key: admin-password
              name: grafana-secret-55dh9ff969
        - name: REQ_URL
          value: http://localhost:3000/api/admin/provisioning/datasources/reload
        - name: REQ_METHOD
          value: POST
        image: quay.io/kiwigrid/k8s-sidecar:1.25.2
        imagePullPolicy: IfNotPresent
        name: grafana-sc-datasources
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /etc/grafana/provisioning/datasources
          name: sc-datasources-volume
      - env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: GF_SECURITY_ADMIN_USER
          valueFrom:
            secretKeyRef:
              key: admin-user
              name: grafana-secret-55dh9ff969
        - name: GF_SECURITY_ADMIN_PASSWORD
          valueFrom:
            secretKeyRef:
              key: admin-password
              name: grafana-secret-55dh9ff969
        - name: GF_PATHS_DATA
          value: /var/lib/grafana/
        - name: GF_PATHS_LOGS
          value: /var/log/grafana
        - name: GF_PATHS_PLUGINS
          value: /var/lib/grafana/plugins
        - name: GF_PATHS_PROVISIONING
          value: /etc/grafana/provisioning
        envFrom:
        - configMapRef:
            name: grafana-env-dkmb477tfh
            optional: true
        image: docker.io/grafana/grafana:10.2.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 10
          httpGet:
            path: /api/health
            port: 3000
          initialDelaySeconds: 60
          timeoutSeconds: 30
        name: grafana
        ports:
        - containerPort: 3000
          name: grafana
          protocol: TCP
        - containerPort: 9094
          name: gossip-tcp
          protocol: TCP
        - containerPort: 9094
          name: gossip-udp
          protocol: UDP
        readinessProbe:
          httpGet:
            path: /api/health
            port: 3000
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /etc/grafana/grafana.ini
          name: config
          subPath: grafana.ini
        - mountPath: /var/lib/grafana
          name: storage
        - mountPath: /dashboards
          name: sc-dashboard-volume
        - mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml
          name: sc-dashboard-provider
          subPath: provider.yaml
        - mountPath: /etc/grafana/provisioning/datasources
          name: sc-datasources-volume
      enableServiceLinks: true
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsNonRoot: true
        runAsUser: 472
      serviceAccountName: grafana
      volumes:
      - configMap:
          name: grafana-8bg2h9g669
        name: config
      - emptyDir: {}
        name: storage
      - emptyDir: {}
        name: sc-dashboard-volume
      - configMap:
          name: grafana-config-dashboards
        name: sc-dashboard-provider
      - emptyDir: {}
        name: sc-datasources-volume
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: minio
    team: team-infra
  name: minio
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
      team: team-infra
  template:
    metadata:
      annotations:
        prometheus.io.path: /minio/v2/metrics/cluster
        prometheus.io.port: "9000"
        prometheus.io.scrape: "true"
      labels:
        app: minio
        team: team-infra
    spec:
      containers:
      - command:
        - sh
        - -c
        - |
          mkdir -p /data/mimir-data /data/mimir-rules /data/mimir-alertmanager && \
          mkdir -p /data/loki-data /data/loki-rules && \
          mkdir -p /data/tempo-data  && \
          mkdir -p /data/pyroscope-data && \
          minio server /data --console-address ':9001'
        env:
        - name: MINIO_PROMETHEUS_AUTH_TYPE
          value: public
        - name: MINIO_ROOT_USER
          value: admin
        - name: MINIO_ROOT_PASSWORD
          value: admin_password
        - name: MINIO_UPDATE
          value: "off"
        image: minio/minio:RELEASE.2023-07-21T21-12-44Z
        imagePullPolicy: IfNotPresent
        name: minio
        ports:
        - containerPort: 9000
          name: http-metrics
        - containerPort: 9001
          name: http-console
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
    team: team-infra
  name: nginx
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
      team: team-infra
  template:
    metadata:
      labels:
        app: nginx
        team: team-infra
    spec:
      containers:
      - envFrom:
        - configMapRef:
            name: nginx-env-d58ddffg6h
        image: nginxinc/nginx-unprivileged:1.25-alpine
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 8080
          name: http-service
        readinessProbe:
          httpGet:
            path: /
            port: http-service
          initialDelaySeconds: 15
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /etc/nginx/templates
          name: templates
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          name: nginx-templates-h69cm5877t
        name: templates
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: overrides-exporter
    team: team-infra
  name: overrides-exporter
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: overrides-exporter
      team: team-infra
  template:
    metadata:
      labels:
        app: overrides-exporter
        team: team-infra
    spec:
      containers:
      - args:
        - -target=overrides-exporter
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: overrides-exporter
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-overrides
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 60
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/instance: prometheus-blackbox-exporter
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: prometheus-blackbox-exporter
    app.kubernetes.io/version: v0.24.0
    helm.sh/chart: prometheus-blackbox-exporter-8.4.0
    team: team-infra
  name: prometheus-blackbox-exporter
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: prometheus-blackbox-exporter
      app.kubernetes.io/name: prometheus-blackbox-exporter
      team: team-infra
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 7af4981bbd5242641843c724fd5fd6b2de154a64e7fcfcddc55be1fb8099e6a9
      labels:
        app.kubernetes.io/instance: prometheus-blackbox-exporter
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: prometheus-blackbox-exporter
        app.kubernetes.io/version: v0.24.0
        helm.sh/chart: prometheus-blackbox-exporter-8.4.0
        team: team-infra
    spec:
      automountServiceAccountToken: false
      containers:
      - args:
        - --config.file=/config/blackbox.yaml
        env: null
        image: quay.io/prometheus/blackbox-exporter:v0.24.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /-/healthy
            port: http
        name: blackbox-exporter
        ports:
        - containerPort: 9115
          name: http
        readinessProbe:
          httpGet:
            path: /-/healthy
            port: http
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        volumeMounts:
        - mountPath: /config
          name: config
      hostNetwork: false
      restartPolicy: Always
      serviceAccountName: prometheus-blackbox-exporter
      volumes:
      - configMap:
          name: prometheus-blackbox-exporter
        name: config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: querier
    team: team-infra
  name: querier
  namespace: monitoring-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app: querier
      team: team-infra
  template:
    metadata:
      labels:
        app: querier
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=querier
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: querier
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-querier
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: query-frontend
    team: team-infra
  name: query-frontend
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: query-frontend
      team: team-infra
  template:
    metadata:
      labels:
        app: query-frontend
        team: team-infra
    spec:
      containers:
      - args:
        - -target=query-frontend
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: query-frontend
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-frontend
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: query-scheduler
    team: team-infra
  name: query-scheduler
  namespace: monitoring-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app: query-scheduler
      team: team-infra
  template:
    metadata:
      labels:
        app: query-scheduler
        team: team-infra
    spec:
      containers:
      - args:
        - -target=query-scheduler
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -server.grpc.keepalive.max-connection-age=2562047h
        - -server.grpc.keepalive.max-connection-age-grace=2562047h
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: query-scheduler
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-scheduler
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ruler
    team: team-infra
  name: ruler
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ruler
      team: team-infra
  template:
    metadata:
      labels:
        app: ruler
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=ruler
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: ruler
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-ruler
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /rules
          name: rule-path
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
      - emptyDir: {}
        name: rule-path
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: alertmanager
    team: team-infra
  name: alertmanager
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
      team: team-infra
  serviceName: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=alertmanager
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: alertmanager
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-am
        - containerPort: 9093
          name: http-web
        - containerPort: 9094
          name: tcp-cluster
        - containerPort: 9094
          name: ucp-cluster
          protocol: UDP
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 60
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: compactor
    team: team-infra
  name: compactor
  namespace: monitoring-system
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app: compactor
      team: team-infra
  serviceName: compactor
  template:
    metadata:
      labels:
        app: compactor
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=compactor
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: compactor
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-compactor
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 240
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
spec:
  minReadySeconds: 10
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: grafana-agent
      app.kubernetes.io/name: grafana-agent
      team: team-infra
  serviceName: grafana-agent
  template:
    metadata:
      annotations:
        metrics.agent.grafana.com/interval: 15s
        metrics.agent.grafana.com/scrape: "true"
      labels:
        app.kubernetes.io/instance: grafana-agent
        app.kubernetes.io/name: grafana-agent
        team: team-infra
    spec:
      containers:
      - args:
        - run
        - /etc/agent/metrics.river
        - --storage.path=/tmp/agent
        - --server.http.listen-addr=0.0.0.0:80
        - --server.http.ui-path-prefix=/
        - --disable-reporting
        - --cluster.enabled=true
        - --cluster.join-addresses=grafana-agent-cluster
        env:
        - name: AGENT_MODE
          value: flow
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        image: docker.io/grafana/agent:v0.37.4
        imagePullPolicy: IfNotPresent
        name: grafana-agent
        ports:
        - containerPort: 80
          name: http-metrics
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 80
          initialDelaySeconds: 10
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /etc/agent
          name: config
      - args:
        - --volume-dir=/etc/agent
        - --webhook-url=http://localhost:80/-/reload
        image: docker.io/jimmidyson/configmap-reload:v0.8.0
        name: config-reloader
        resources:
          requests:
            cpu: 1m
            memory: 5Mi
        volumeMounts:
        - mountPath: /etc/agent
          name: config
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: grafana-agent
      volumes:
      - configMap:
          name: grafana-agent-metrics-config-m75g754mc5
        name: config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: ingester
    team: team-infra
  name: ingester
  namespace: monitoring-system
spec:
  podManagementPolicy: Parallel
  replicas: 2
  selector:
    matchLabels:
      app: ingester
      team: team-infra
  serviceName: ingester-headless
  template:
    metadata:
      labels:
        app: ingester
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=ingester
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -ingester.ring.instance-availability-zone=zone-default
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: ingester
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-ingester
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 240
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: store-gateway
    team: team-infra
  name: store-gateway
  namespace: monitoring-system
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app: store-gateway
      team: team-infra
  serviceName: store-gateway-headless
  template:
    metadata:
      labels:
        app: store-gateway
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=store-gateway
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.3
        imagePullPolicy: IfNotPresent
        name: store-gateway
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc-store-gw
          protocol: TCP
        - containerPort: 7946
          name: http-memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 240
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-d6979bkdgd
        name: runtime-config
