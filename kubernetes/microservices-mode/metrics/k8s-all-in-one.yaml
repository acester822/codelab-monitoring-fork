apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
rules:
- apiGroups:
  - ""
  - discovery.k8s.io
  - networking.k8s.io
  resources:
  - endpoints
  - endpointslices
  - ingresses
  - nodes
  - nodes/proxy
  - nodes/metrics
  - pods
  - services
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - pods
  - pods/log
  - namespaces
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.grafana.com
  resources:
  - podlogs
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - monitoring.coreos.com
  resources:
  - prometheusrules
  verbs:
  - get
  - list
  - watch
- nonResourceURLs:
  - /metrics
  verbs:
  - get
- apiGroups:
  - monitoring.coreos.com
  resources:
  - podmonitors
  - servicemonitors
  - probes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - configmaps
  - secrets
  verbs:
  - get
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: grafana-agent
subjects:
- kind: ServiceAccount
  name: grafana-agent
  namespace: monitoring-system
---
apiVersion: v1
data:
  metrics.river: "logging {\n  level  = coalesce(env(\"AGENT_LOG_LEVEL\"), \"info\")\n
    \ format = \"logfmt\"\n}\n\nmodule.file \"lgtmp\" {\n\tfilename = env(\"AGENT_CONFIG_FOLDER\")
    + \"/lgtmp.river\"\n\n\targuments {\n    cluster          = coalesce(env(\"CLUSTER\"),
    \"k3d-k3s-codelab\")\n    metrics_endpoint = coalesce(env(\"METRICS_ENDPOINT\"),
    \"http://nginx.monitoring-system:8080\")\n\t}\n}\n\nmodule.file \"metrics_primary\"
    {\n\tfilename = env(\"AGENT_CONFIG_FOLDER\") + \"/metrics-all.river\"\n\n  arguments
    {\n    forward_to = [module.file.lgtmp.exports.metrics_receiver]\n    clustering
    = true\n  }\n}"
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: agent-metrics-config-2dch2ttm97
  namespace: monitoring-system
---
apiVersion: v1
data:
  grafana-cloud.river: "\n/********************************************\n * ARGUMENTS\n
    ********************************************/\nargument \"stack_name\" { }\n\nargument
    \"token\" { }\n\n/********************************************\n * EXPORTS\n ********************************************/\n\nexport
    \"metrics_receiver\" {\n\tvalue = prometheus.remote_write.default.receiver\n}\n\nexport
    \"logs_receiver\" {\n\tvalue = loki.write.default.receiver\n}\n\nexport \"traces_receiver\"
    {\n\tvalue = otelcol.exporter.otlp.default.input\n}\n\nexport \"profiles_receiver\"
    {\n\tvalue = pyroscope.write.default.receiver\n}\n\nexport \"stack_information\"
    {\n\tvalue = json_decode(remote.http.config_file.content)\n}\n\n/********************************************\n
    * External information\n ********************************************/\n\nremote.http
    \"config_file\" {\n\turl = \"https://grafana.com/api/instances/\" + argument.stack_name.value\n\n\tclient
    {\n\t\tbearer_token = argument.token.value\n\t}\n\tpoll_frequency = \"24h\"\n}\n\n/********************************************\n
    * Endpoints\n ********************************************/\n\n// Metrics\nprometheus.remote_write
    \"default\" {\n\tendpoint {\n\t\turl = json_decode(remote.http.config_file.content)[\"hmInstancePromUrl\"]
    + \"/api/prom/push\"\n\n\t\tbasic_auth {\n\t\t\tusername = json_decode(remote.http.config_file.content)[\"hmInstancePromId\"]\n\t\t\tpassword
    = argument.token.value\n\t\t}\n\t}\n}\n\n// Logs\nloki.write \"default\" {\n\tendpoint
    {\n\t\turl = json_decode(remote.http.config_file.content)[\"hlInstanceUrl\"] +
    \"/loki/api/v1/push\"\n\n\t\tbasic_auth {\n\t\t\tusername = json_decode(remote.http.config_file.content)[\"hlInstanceId\"]\n\t\t\tpassword
    = argument.token.value\n\t\t}\n\t}\n}\n\n// Traces\notelcol.auth.basic \"default\"
    {\n\tusername = json_decode(remote.http.config_file.content)[\"htInstanceId\"]\n\tpassword
    = argument.token.value\n}\n\notelcol.exporter.otlp \"default\" {\n\tclient {\n\t\tendpoint
    = json_decode(remote.http.config_file.content)[\"htInstanceUrl\"] + \":443\"\n\t\tauth
    \    = otelcol.auth.basic.default.handler\n\t}\n}\n\n// Profiles\npyroscope.write
    \"default\" {\n\tendpoint {\n\t\turl = json_decode(remote.http.config_file.content)[\"hpInstanceUrl\"]\n\n\t\tbasic_auth
    {\n\t\t\tusername = json_decode(remote.http.config_file.content)[\"hpInstanceId\"]\n\t\t\tpassword
    = argument.token.value\n\t\t}\n\t}\n}\n"
  lgtmp.river: "/********************************************\n * ARGUMENTS\n ********************************************/\nargument
    \"cluster\" {\n\toptional = true\n\tdefault  = \"monitoring-system\"\n}\n\nargument
    \"metrics_endpoint\" {\n\toptional = true\n\tdefault  = \"http://mimir:8080\"\n\t//comment
    = \"Where to send collected metrics.\"\n}\n\nargument \"logs_endpoint\" {\n\toptional
    = true\n\tdefault  = \"http://loki:3100\"\n\t//comment = \"Where to send collected
    logs.\"\n}\n\nargument \"traces_endpoint\" {\n\toptional = true\n\tdefault  =
    \"tempo:4317\"\n\t//comment = \"Where to send collected traces.\"\n}\n\nargument
    \"profiles_endpoint\" {\n\toptional = true\n\tdefault  = \"http://pyroscope:4040\"\n\t//comment
    \ = \"Where to send collected profiles.\"\n}\n\n/********************************************\n
    * EXPORTS\n ********************************************/\n\nexport \"metrics_receiver\"
    {\n\tvalue = prometheus.remote_write.mimir.receiver\n}\n\nexport \"logs_receiver\"
    {\n\tvalue = loki.write.loki.receiver\n}\n\nexport \"traces_receiver\" {\n\tvalue
    = otelcol.exporter.otlp.tempo.input\n}\n\nexport \"profiles_receiver\" {\n\tvalue
    = pyroscope.write.pyroscope.receiver\n}\n\n/********************************************\n
    * Endpoints\n ********************************************/\n\n// Metrics\nprometheus.remote_write
    \"mimir\" {\n\tendpoint {\n\t\turl = argument.metrics_endpoint.value + \"/api/v1/push\"\n\t}\n\n\texternal_labels
    = {\n\t\t\"scraped_by\" = \"grafana-agent\",\n\t\t\"cluster\" \t = argument.cluster.value,\n\t}\n}\n\n//
    Logs\nloki.write \"loki\" {\n\tendpoint {\n\t\turl = argument.logs_endpoint.value
    + \"/loki/api/v1/push\"\n\t}\n\n\texternal_labels = {\n\t\t\"scraped_by\" = \"grafana-agent\",\n\t\t\"cluster\"
    \t = argument.cluster.value,\n\t}\n}\n\n// Traces\notelcol.exporter.otlp \"tempo\"
    {\n\tclient {\n\t\tendpoint = argument.traces_endpoint.value\n\n\t\ttls {\n\t\t\tinsecure
    \            = true\n\t\t\tinsecure_skip_verify = true\n\t\t}\n\t}\n}\n\n// Profiles\npyroscope.write
    \"pyroscope\" {\n\tendpoint {\n\t\turl = argument.profiles_endpoint.value\n\t}\n\n\texternal_labels
    = {\n\t\t\"scraped_by\" = \"grafana-agent\",\n\t\t\"cluster\" \t = argument.cluster.value,\n\t}\n}\n"
  metrics-all.river: "/*\nModule: metrics-all\nDescription: Wrapper module to include
    all kubernetes metric modules and use cri parsing\n*/\nargument \"forward_to\"
    {\n  // comment = \"Must be a list(MetricssReceiver) where collected logs should
    be forwarded to\"\n\toptional = false\n}\n\nargument \"clustering\" {\n  // comment
    = \"Whether or not clustering should be enabled\"\n  optional = true\n  default
    = false\n}\n\n/********************************************\n * Kubernetes Auto
    Scrape ServiceMonitor\n ********************************************/\nprometheus.operator.servicemonitors
    \"auto_scrape_servicemonitors\" {\n  forward_to = argument.forward_to.value\n
    \ \n  clustering {\n    enabled = argument.clustering.value\n  }\n}\n\n\n/********************************************\n
    * Kubernetes Auto Scrape PodMonitors\n ********************************************/\nprometheus.operator.podmonitors
    \"auto_scrape_podmonitors\" {\n  forward_to = argument.forward_to.value\n    \n
    \ clustering {\n    enabled = argument.clustering.value\n  }\n  \n  selector {\n
    \     match_expression {\n          key = \"team\"\n          operator = \"In\"\n
    \         values = [\"team-infra\"]\n      }\n  }\n}\n\n/********************************************\n
    * Kubernetes Prometheus Rules To Mimir\n ********************************************/\nmimir.rules.kubernetes
    \"prometheus_rules_to_mimir\" {\n    address = coalesce(env(\"METRICS_ENDPOINT\"),
    \"http://nginx.monitoring-system:8080\")\n    tenant_id = \"anonymous\"\n}"
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: agent-modules-42g2925446
  namespace: monitoring-system
---
apiVersion: v1
data:
  datasources.yaml: |
    apiVersion: 1

    datasources:
    # Mimir for metrics
    - name: Mimir
      type: prometheus
      uid: mimir
      access: proxy
      orgId: 1
      url: http://nginx:8080/prometheus
      basicAuth: false
      isDefault: true
      version: 1
      editable: true
kind: ConfigMap
metadata:
  labels:
    grafana_datasource: "1"
    team: team-infra
  name: grafana-datasources-mimir
  namespace: monitoring-system
---
apiVersion: v1
data:
  ALERT_MANAGER_HOST: alertmanager-headless.monitoring-system.svc.cluster.local
  COMPACTOR_HOST: compactor.monitoring-system.svc.cluster.local
  DISTRIBUTOR_HOST: distributor.monitoring-system.svc.cluster.local
  NGINX_ENVSUBST_OUTPUT_DIR: /etc/nginx
  QUERY_FRONTEND_HOST: query-frontend.monitoring-system.svc.cluster.local
  RULER_HOST: ruler.monitoring-system.svc.cluster.local
immutable: true
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: nginx-env-d58ddffg6h
  namespace: monitoring-system
---
apiVersion: v1
data:
  gateway_mimir.conf.template: "server {\n    listen 8080;\n    listen [::]:8080;\n\n
    \   location = / {\n      return 200 'OK';\n      auth_basic off;\n      access_log
    off;\n    }\n\n    proxy_set_header X-Scope-OrgID $ensured_x_scope_orgid;\n\n
    \   # Distributor endpoints\n    location /distributor {\n      proxy_pass      http://${DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n    location = /api/v1/push {\n      proxy_pass      http://${DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n    location /otlp/v1/metrics {\n      proxy_pass      http://${DISTRIBUTOR_HOST}:8080$request_uri;\n
    \   }\n\n    # Alertmanager endpoints\n    location /alertmanager {\n      proxy_pass
    \     http://${ALERT_MANAGER_HOST}:8080$request_uri;\n    }\n    location = /multitenant_alertmanager/status
    {\n      proxy_pass      http://${ALERT_MANAGER_HOST}:8080$request_uri;\n    }\n
    \   location = /api/v1/alerts {\n      proxy_pass      http://${ALERT_MANAGER_HOST}:8080$request_uri;\n
    \   }\n\n    # Ruler endpoints\n    location /prometheus/config/v1/rules {\n      proxy_pass
    \     http://${RULER_HOST}:8080$request_uri;\n    }\n    location /prometheus/api/v1/rules
    {\n      proxy_pass      http://${RULER_HOST}:8080$request_uri;\n    }\n    \n
    \   location /prometheus/api/v1/alerts {\n      proxy_pass      http://${RULER_HOST}:8080$request_uri;\n
    \   }\n    location = /ruler/ring {\n      proxy_pass      http://${RULER_HOST}:8080$request_uri;\n
    \   }\n\n    # Rest of /prometheus goes to the query frontend\n    location /prometheus
    {\n      proxy_pass      http://${QUERY_FRONTEND_HOST}:8080$request_uri;\n    }\n\n
    \   # Buildinfo endpoint can go to any component\n    location = /api/v1/status/buildinfo
    {\n      proxy_pass      http://${QUERY_FRONTEND_HOST}:8080$request_uri;\n    }\n\n
    \   # Compactor endpoint for uploading blocks\n    location /api/v1/upload/block/
    {\n      proxy_pass      http://${COMPACTOR_HOST}:8080$request_uri;\n    }\n}"
  nginx.conf.template: |-
    worker_processes  auto;
    error_log  /dev/stderr error;
    pid        /tmp/nginx.pid;
    worker_rlimit_nofile 8192;

    events {
      worker_connections  4096;  ## Default: 1024
    }

    http {
      client_body_temp_path /tmp/client_temp;
      proxy_temp_path       /tmp/proxy_temp_path;
      fastcgi_temp_path     /tmp/fastcgi_temp;
      uwsgi_temp_path       /tmp/uwsgi_temp;
      scgi_temp_path        /tmp/scgi_temp;

      client_max_body_size  4M;

      proxy_read_timeout    600; ## 10 minutes
      proxy_send_timeout    600;
      proxy_connect_timeout 600;

      proxy_http_version    1.1;

      default_type application/octet-stream;
      log_format   main '$remote_addr - $remote_user [$time_local]  $status '
            '"$request" $body_bytes_sent "$http_referer" '
            '"$http_user_agent" "$http_x_forwarded_for"';

      map $status $loggable {
        ~^[23]  0;
        default 1;
      }

      access_log   /dev/stderr  main if=$loggable;

      sendfile     on;
      tcp_nopush   on;

      resolver kube-dns.kube-system.svc.cluster.local;

      # Ensure that X-Scope-OrgID is always present, default to the no_auth_tenant for backwards compatibility when multi-tenancy was turned off.
      map $http_x_scope_orgid $ensured_x_scope_orgid {
        default $http_x_scope_orgid;
        "" "anonymous";
      }

      include /etc/nginx/gateway_*.conf;
    }
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: nginx-templates-h69cm5877t
  namespace: monitoring-system
---
apiVersion: v1
data:
  runtime.yaml: |-
    # https://grafana.com/docs/mimir/latest/configure/about-runtime-configuration/
    ingester_limits: # limits that each ingester replica enforces
      max_ingestion_rate: 20000
      max_series: 1500000
      max_tenants: 1000
      max_inflight_push_requests: 30000

    distributor_limits: # limits that each distributor replica enforces
      max_ingestion_rate: 20000
      max_inflight_push_requests: 30000
      max_inflight_push_requests_bytes: 50000000

    overrides:
      anonymous: # limits for anonymous that the whole cluster enforces
        # ingestion_tenant_shard_size: 9
        max_global_series_per_user: 1500000
        max_global_series_per_metric: 50000
        max_fetched_series_per_query: 100000
        ruler_max_rules_per_rule_group: 100
        ruler_max_rule_groups_per_tenant: 100
immutable: true
kind: ConfigMap
metadata:
  labels:
    team: team-infra
  name: runtime-config-88gg5gk88d
  namespace: monitoring-system
---
apiVersion: v1
data:
  AGENT_CONFIG_FOLDER: L2V0Yy9hZ2VudC1tb2R1bGVz
  AGENT_LOG_LEVEL: aW5mbw==
  CLUSTER: azNkLWszcy1jb2RlbGFi
  METRICS_ENDPOINT: aHR0cDovL25naW54Lm1vbml0b3Jpbmctc3lzdGVtOjgwODA=
kind: Secret
metadata:
  labels:
    team: team-infra
  name: agent-metrics-env-c9685b4855
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
data:
  alertmanager-fallback-config.yaml: |
    cm91dGU6CiAgZ3JvdXBfd2FpdDogMHMKICByZWNlaXZlcjogZW1wdHktcmVjZWl2ZXIKCn
    JlY2VpdmVyczoKICAjIEluIHRoaXMgZXhhbXBsZSB3ZSdyZSBub3QgZ29pbmcgdG8gc2Vu
    ZCBhbnkgbm90aWZpY2F0aW9uIG91dCBvZiBBbGVydG1hbmFnZXIuCiAgLSBuYW1lOiAnZW
    1wdHktcmVjZWl2ZXInCg==
  mimir.yaml: |
    IyBEbyBub3QgdXNlIHRoaXMgY29uZmlndXJhdGlvbiBpbiBwcm9kdWN0aW9uLgojIEl0IG
    lzIGZvciBkZW1vbnN0cmF0aW9uIHB1cnBvc2VzIG9ubHkuCm11bHRpdGVuYW5jeV9lbmFi
    bGVkOiBmYWxzZQoKIyAtdXNhZ2Utc3RhdHMuZW5hYmxlZD1mYWxzZQp1c2FnZV9zdGF0cz
    oKICBlbmFibGVkOiBmYWxzZQoKc2VydmVyOgogIGh0dHBfbGlzdGVuX3BvcnQ6IDgwODAK
    ICBncnBjX2xpc3Rlbl9wb3J0OiA5MDk1CiAgbG9nX2xldmVsOiB3YXJuCgpjb21tb246Ci
    Agc3RvcmFnZToKICAgIGJhY2tlbmQ6IHMzCiAgICBzMzoKICAgICAgZW5kcG9pbnQ6ICAg
    ICAgICAgIG1pbmlvOjkwMDAKICAgICAgYWNjZXNzX2tleV9pZDogICAgIGFkbWluCiAgIC
    AgIHNlY3JldF9hY2Nlc3Nfa2V5OiBhZG1pbl9wYXNzd29yZAogICAgICBpbnNlY3VyZTog
    ICAgICAgICAgdHJ1ZQoKYWxlcnRtYW5hZ2VyOgogIGRhdGFfZGlyOiAvZGF0YS9hbGVydG
    1hbmFnZXIKICBlbmFibGVfYXBpOiB0cnVlCiAgZXh0ZXJuYWxfdXJsOiAvYWxlcnRtYW5h
    Z2VyCiAgZmFsbGJhY2tfY29uZmlnX2ZpbGU6IC9ldGMvbWltaXIvYWxlcnRtYW5hZ2VyLW
    ZhbGxiYWNrLWNvbmZpZy55YW1sCmFsZXJ0bWFuYWdlcl9zdG9yYWdlOgogIHMzOgogICAg
    YnVja2V0X25hbWU6IG1pbWlyLWFsZXJ0bWFuYWdlcgoKYmxvY2tzX3N0b3JhZ2U6CiAgcz
    M6CiAgICBidWNrZXRfbmFtZTogbWltaXItZGF0YQogIHRzZGI6CiAgICBkaXI6IC9kYXRh
    L2luZ2VzdGVyCgpmcm9udGVuZDoKICBwYXJhbGxlbGl6ZV9zaGFyZGFibGVfcXVlcmllcz
    ogdHJ1ZQogIHNjaGVkdWxlcl9hZGRyZXNzOiBxdWVyeS1zY2hlZHVsZXItaGVhZGxlc3M6
    OTA5NQpmcm9udGVuZF93b3JrZXI6CiAgZ3JwY19jbGllbnRfY29uZmlnOgogICAgbWF4X3
    NlbmRfbXNnX3NpemU6IDQxOTQzMDQwMAogIHNjaGVkdWxlcl9hZGRyZXNzOiBxdWVyeS1z
    Y2hlZHVsZXItaGVhZGxlc3M6OTA5NQoKbWVtYmVybGlzdDoKICBqb2luX21lbWJlcnM6IF
    sgZ29zc2lwLXJpbmctaGVhZGxlc3M6Nzk0NiBdCgpydWxlcjoKICBydWxlX3BhdGg6IC9k
    YXRhL3J1bGVzCiAgZW5hYmxlX2FwaTogdHJ1ZQogIGFsZXJ0bWFuYWdlcl91cmw6IGh0dH
    A6Ly9hbGVydG1hbmFnZXItaGVhZGxlc3M6ODA4MC9hbGVydG1hbmFnZXIKcnVsZXJfc3Rv
    cmFnZToKICBzMzoKICAgIGJ1Y2tldF9uYW1lOiBtaW1pci1ydWxlcwoKcnVudGltZV9jb2
    5maWc6CiAgZmlsZTogL3Zhci9taW1pci9ydW50aW1lLnlhbWwKCnF1ZXJ5X3NjaGVkdWxl
    cjoKICBtYXhfb3V0c3RhbmRpbmdfcmVxdWVzdHNfcGVyX3RlbmFudDogODAwCgpsaW1pdH
    M6CiAgbmF0aXZlX2hpc3RvZ3JhbXNfaW5nZXN0aW9uX2VuYWJsZWQ6IHRydWUK
kind: Secret
metadata:
  labels:
    team: team-infra
  name: mimir-config-6f5cgttm66
  namespace: monitoring-system
type: Opaque
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: alertmanager
    team: team-infra
  name: alertmanager
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-am
    port: 9095
  selector:
    app: alertmanager
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: alertmanager
    prometheus.io/service-monitor: "false"
    team: team-infra
  name: alertmanager-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: http-web
    port: 9093
  - name: tcp-cluster
    port: 9094
  - name: udp-cluster
    port: 9094
    protocol: UDP
  - name: grpc-am
    port: 9095
  publishNotReadyAddresses: true
  selector:
    app: alertmanager
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: compactor
    team: team-infra
  name: compactor
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-compactor
    port: 9095
  selector:
    app: compactor
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: distributor
    team: team-infra
  name: distributor
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-distribut
    port: 9095
  selector:
    app: distributor
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: gossip-ring
    prometheus.io/service-monitor: "false"
    team: team-infra
  name: gossip-ring-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: tcp-gossip-ring
    port: 7946
    protocol: TCP
    targetPort: 7946
  publishNotReadyAddresses: true
  selector:
    gossip_ring_member: "true"
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/name: grafana-agent
    team: team-infra
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent-cluster
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/name: grafana-agent
    team: team-infra
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingester
    team: team-infra
  name: ingester
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-ingester
    port: 9095
  selector:
    app: ingester
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ingester
    prometheus.io/service-monitor: "false"
    team: team-infra
  name: ingester-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-ingester
    port: 9095
  selector:
    app: ingester
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: minio
    team: team-infra
  name: minio
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 9000
  selector:
    app: minio
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: nginx
    team: team-infra
  name: nginx
  namespace: monitoring-system
spec:
  ports:
  - name: http-service
    port: 8080
  selector:
    app: nginx
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: overrides-exporter
    team: team-infra
  name: overrides-exporter
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-overrides
    port: 9095
  selector:
    app: overrides-exporter
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: querier
    team: team-infra
  name: querier
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-querier
    port: 9095
  selector:
    app: querier
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: query-frontend
    team: team-infra
  name: query-frontend
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-frontend
    port: 9095
  selector:
    app: query-frontend
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: query-scheduler
    team: team-infra
  name: query-scheduler
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-scheduler
    port: 9095
  selector:
    app: query-scheduler
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: query-scheduler
    prometheus.io/service-monitor: "false"
    team: team-infra
  name: query-scheduler-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-scheduler
    port: 9095
  publishNotReadyAddresses: true
  selector:
    app: query-scheduler
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ruler
    team: team-infra
  name: ruler
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  selector:
    app: ruler
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: store-gateway
    team: team-infra
  name: store-gateway
  namespace: monitoring-system
spec:
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-store-gw
    port: 9095
  selector:
    app: store-gateway
    team: team-infra
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: store-gateway
    prometheus.io/service-monitor: "false"
    team: team-infra
  name: store-gateway-headless
  namespace: monitoring-system
spec:
  clusterIP: None
  ports:
  - name: http-metrics
    port: 8080
  - name: grpc-store-gw
    port: 9095
  selector:
    app: store-gateway
    team: team-infra
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: distributor
    team: team-infra
  name: distributor
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: distributor
      team: team-infra
  template:
    metadata:
      labels:
        app: distributor
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=distributor
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.4
        imagePullPolicy: IfNotPresent
        name: distributor
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-distribut
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 60
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-88gg5gk88d
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: minio
    team: team-infra
  name: minio
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: minio
      team: team-infra
  template:
    metadata:
      annotations:
        prometheus.io.path: /minio/v2/metrics/cluster
        prometheus.io.port: "9000"
        prometheus.io.scrape: "false"
      labels:
        app: minio
        team: team-infra
    spec:
      containers:
      - command:
        - sh
        - -c
        - |
          mkdir -p /data/mimir-data /data/mimir-rules /data/mimir-alertmanager && \
          mkdir -p /data/loki-data /data/loki-rules && \
          mkdir -p /data/tempo-data  && \
          mkdir -p /data/pyroscope-data && \
          minio server /data --console-address ':9001'
        env:
        - name: MINIO_PROMETHEUS_AUTH_TYPE
          value: public
        - name: MINIO_ROOT_USER
          value: admin
        - name: MINIO_ROOT_PASSWORD
          value: admin_password
        - name: MINIO_UPDATE
          value: "off"
        image: minio/minio:RELEASE.2023-07-21T21-12-44Z
        imagePullPolicy: IfNotPresent
        name: minio
        ports:
        - containerPort: 9000
          name: http-metrics
        - containerPort: 9001
          name: http-console
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
    team: team-infra
  name: nginx
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
      team: team-infra
  template:
    metadata:
      labels:
        app: nginx
        team: team-infra
    spec:
      containers:
      - envFrom:
        - configMapRef:
            name: nginx-env-d58ddffg6h
        image: nginxinc/nginx-unprivileged:1.25-alpine
        imagePullPolicy: IfNotPresent
        name: nginx
        ports:
        - containerPort: 8080
          name: http-service
        readinessProbe:
          httpGet:
            path: /
            port: http-service
          initialDelaySeconds: 15
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /etc/nginx/templates
          name: templates
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          name: nginx-templates-h69cm5877t
        name: templates
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: overrides-exporter
    team: team-infra
  name: overrides-exporter
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: overrides-exporter
      team: team-infra
  template:
    metadata:
      labels:
        app: overrides-exporter
        team: team-infra
    spec:
      containers:
      - args:
        - -target=overrides-exporter
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        image: grafana/mimir:2.10.4
        imagePullPolicy: IfNotPresent
        name: overrides-exporter
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-overrides
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 60
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-88gg5gk88d
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: querier
    team: team-infra
  name: querier
  namespace: monitoring-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app: querier
      team: team-infra
  template:
    metadata:
      labels:
        app: querier
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=querier
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.4
        imagePullPolicy: IfNotPresent
        name: querier
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-querier
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-88gg5gk88d
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: query-frontend
    team: team-infra
  name: query-frontend
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: query-frontend
      team: team-infra
  template:
    metadata:
      labels:
        app: query-frontend
        team: team-infra
    spec:
      containers:
      - args:
        - -target=query-frontend
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        image: grafana/mimir:2.10.4
        imagePullPolicy: IfNotPresent
        name: query-frontend
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-frontend
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-88gg5gk88d
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: query-scheduler
    team: team-infra
  name: query-scheduler
  namespace: monitoring-system
spec:
  replicas: 2
  selector:
    matchLabels:
      app: query-scheduler
      team: team-infra
  template:
    metadata:
      labels:
        app: query-scheduler
        team: team-infra
    spec:
      containers:
      - args:
        - -target=query-scheduler
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -server.grpc.keepalive.max-connection-age=2562047h
        - -server.grpc.keepalive.max-connection-age-grace=2562047h
        image: grafana/mimir:2.10.4
        imagePullPolicy: IfNotPresent
        name: query-scheduler
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-scheduler
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-88gg5gk88d
        name: runtime-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ruler
    team: team-infra
  name: ruler
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ruler
      team: team-infra
  template:
    metadata:
      labels:
        app: ruler
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=ruler
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.4
        imagePullPolicy: IfNotPresent
        name: ruler
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-ruler
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
        - mountPath: /rules
          name: rule-path
      terminationGracePeriodSeconds: 180
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-88gg5gk88d
        name: runtime-config
      - emptyDir: {}
        name: rule-path
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: alertmanager
    team: team-infra
  name: alertmanager
  namespace: monitoring-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
      team: team-infra
  serviceName: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=alertmanager
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.4
        imagePullPolicy: IfNotPresent
        name: alertmanager
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-am
        - containerPort: 9093
          name: http-web
        - containerPort: 9094
          name: tcp-cluster
        - containerPort: 9094
          name: ucp-cluster
          protocol: UDP
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 45
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 60
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-88gg5gk88d
        name: runtime-config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: compactor
    team: team-infra
  name: compactor
  namespace: monitoring-system
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app: compactor
      team: team-infra
  serviceName: compactor
  template:
    metadata:
      labels:
        app: compactor
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=compactor
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.4
        imagePullPolicy: IfNotPresent
        name: compactor
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-compactor
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 240
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-88gg5gk88d
        name: runtime-config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
spec:
  minReadySeconds: 10
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: grafana-agent
      app.kubernetes.io/name: grafana-agent
      team: team-infra
  serviceName: grafana-agent
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: grafana-agent
        app.kubernetes.io/name: grafana-agent
        team: team-infra
    spec:
      containers:
      - args:
        - run
        - /etc/agent/metrics.river
        - --storage.path=/tmp/agent
        - --server.http.listen-addr=0.0.0.0:80
        - --server.http.ui-path-prefix=/
        - --disable-reporting
        - --cluster.enabled=true
        - --cluster.join-addresses=grafana-agent-cluster
        env:
        - name: AGENT_MODE
          value: flow
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        envFrom:
        - secretRef:
            name: agent-metrics-env-c9685b4855
        image: docker.io/grafana/agent:v0.37.4
        imagePullPolicy: IfNotPresent
        name: grafana-agent
        ports:
        - containerPort: 80
          name: http-metrics
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 80
          initialDelaySeconds: 10
          timeoutSeconds: 1
        volumeMounts:
        - mountPath: /etc/agent
          name: config
        - mountPath: /etc/agent-modules
          name: agent-modules
      - args:
        - --volume-dir=/etc/agent
        - --webhook-url=http://localhost:80/-/reload
        image: docker.io/jimmidyson/configmap-reload:v0.8.0
        name: config-reloader
        resources:
          requests:
            cpu: 1m
            memory: 5Mi
        volumeMounts:
        - mountPath: /etc/agent
          name: config
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: grafana-agent
      volumes:
      - configMap:
          name: agent-metrics-config-2dch2ttm97
        name: config
      - configMap:
          name: agent-modules-42g2925446
        name: agent-modules
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: ingester
    team: team-infra
  name: ingester
  namespace: monitoring-system
spec:
  podManagementPolicy: Parallel
  replicas: 2
  selector:
    matchLabels:
      app: ingester
      team: team-infra
  serviceName: ingester-headless
  template:
    metadata:
      labels:
        app: ingester
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=ingester
        - -config.expand-env=true
        - -config.file=/etc/mimir/mimir.yaml
        - -ingester.ring.instance-availability-zone=zone-default
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.4
        imagePullPolicy: IfNotPresent
        name: ingester
        ports:
        - containerPort: 8080
          name: http-metrics
        - containerPort: 9095
          name: grpc-ingester
        - containerPort: 7946
          name: http-memberlist
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 240
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-88gg5gk88d
        name: runtime-config
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    app: store-gateway
    team: team-infra
  name: store-gateway
  namespace: monitoring-system
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      app: store-gateway
      team: team-infra
  serviceName: store-gateway-headless
  template:
    metadata:
      labels:
        app: store-gateway
        gossip_ring_member: "true"
        team: team-infra
    spec:
      containers:
      - args:
        - -target=store-gateway
        - -config.file=/etc/mimir/mimir.yaml
        - -config.expand-env=true
        - -memberlist.bind-addr=$(POD_IP)
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        image: grafana/mimir:2.10.4
        imagePullPolicy: IfNotPresent
        name: store-gateway
        ports:
        - containerPort: 8080
          name: http-metrics
          protocol: TCP
        - containerPort: 9095
          name: grpc-store-gw
          protocol: TCP
        - containerPort: 7946
          name: http-memberlist
          protocol: TCP
        readinessProbe:
          httpGet:
            path: /ready
            port: http-metrics
          initialDelaySeconds: 60
        volumeMounts:
        - mountPath: /etc/mimir
          name: mimir-config
        - mountPath: /var/mimir
          name: runtime-config
      terminationGracePeriodSeconds: 240
      volumes:
      - name: mimir-config
        secret:
          secretName: mimir-config-6f5cgttm66
      - configMap:
          name: runtime-config-88gg5gk88d
        name: runtime-config
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: alertmanager
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/alertmanager
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: alertmanager
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: compactor
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/compactor
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: compactor
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: distributor
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/distributor
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: distributor
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
spec:
  endpoints:
  - honorLabels: true
    interval: 15s
    port: http-metrics
  selector:
    matchLabels:
      app.kubernetes.io/instance: grafana-agent
      app.kubernetes.io/name: grafana-agent
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: ingester
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/ingester
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: ingester
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: minio
  namespace: monitoring-system
spec:
  endpoints:
  - path: /minio/v2/metrics/cluster
    port: http-metrics
    relabelings:
    - replacement: monitoring-system/minio
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: minio
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: overrides-exporter
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/overrides-exporter
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: overrides-exporter
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: querier
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/querier
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: querier
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: query-frontend
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/query-frontend
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: query-frontend
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: query-scheduler
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/query-scheduler
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: query-scheduler
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: ruler
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/ruler
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: ruler
---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    team: team-infra
  name: store-gateway
  namespace: monitoring-system
spec:
  endpoints:
  - port: http-metrics
    relabelings:
    - replacement: monitoring-system/store-gateway
      sourceLabels:
      - job
      targetLabel: job
    scheme: http
  namespaceSelector:
    matchNames:
    - monitoring-system
  selector:
    matchExpressions:
    - key: prometheus.io/service-monitor
      operator: NotIn
      values:
      - "false"
    matchLabels:
      app: store-gateway
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/instance: grafana-agent
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: grafana-agent
    app.kubernetes.io/version: v0.37.4
    helm.sh/chart: grafana-agent-0.27.2
    team: team-infra
  name: grafana-agent
  namespace: monitoring-system
spec:
  rules:
  - host: grafana-agent-metrics.localhost
    http:
      paths:
      - backend:
          service:
            name: grafana-agent
            port:
              number: 80
        path: /
        pathType: Prefix
