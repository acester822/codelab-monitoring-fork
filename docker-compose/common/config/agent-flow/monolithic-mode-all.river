// https://github.com/grafana/agent-configurator

logging {
	level  = "warn"
	format = "logfmt"
}

tracing {
	sampling_fraction = 0.8
	write_to          = [otelcol.processor.batch.containers.input]
}

module.file "docker_compose" {
	filename = env("AGENT_CONFIG_FOLDER") + "/modules/docker_compose.river"
}

discovery.relabel "containers" {
	targets = module.file.docker_compose.exports.relabelings_common.output
}

/********************************************
 * Metrics
 ********************************************/

prometheus.exporter.unix "containers" {
	set_collectors     = ["cpu"]
	disable_collectors = ["diskstats", "mdadm", "textfile", "hwmon"]
}

prometheus.scrape "integrations" {
	targets = concat(
		prometheus.exporter.unix.containers.targets,
	)
	scrape_interval = "15s"

	enable_protobuf_negotiation = true
	scrape_classic_histograms   = true

	clustering {
		enabled = true
	}

	forward_to = [prometheus.relabel.integrations.receiver]
}

prometheus.scrape "containers" {
	targets         = discovery.relabel.containers.output
	scrape_interval = "15s"

	enable_protobuf_negotiation = true
	scrape_classic_histograms   = true

	clustering {
		enabled = true
	}

	forward_to = [module.file.docker_compose.exports.metrics_receiver]
}

prometheus.scrape "minio" {
	targets = [{"__address__" = "minio:9000", "job" = "minio-job"}]

	enable_protobuf_negotiation = true
	scrape_classic_histograms   = true

	clustering {
		enabled = true
	}

	scrape_interval = "15s"
	metrics_path    = "/minio/v2/metrics/cluster"

	forward_to = [prometheus.relabel.integrations.receiver]
}

prometheus.relabel "integrations" {
	rule {
		source_labels = ["job"]
		regex         = "(integrations|monitoring-system)/(.*)"
		target_label  = "pod"
		replacement   = "${2}"
	}

	rule {
		source_labels = ["job"]
		regex         = "(integrations|monitoring-system)/(.*)"
		target_label  = "container"
		replacement   = "${2}"
	}

	forward_to = [module.file.docker_compose.exports.metrics_receiver]
}

/********************************************
 * Logs
 ********************************************/

loki.source.docker "containers" {
	host          = "unix:///var/run/docker.sock"
	targets       = discovery.relabel.containers.output
	relabel_rules = discovery.relabel.containers.rules
	forward_to    = [loki.process.containers.receiver]
}

loki.process "containers" {
	stage.drop {
		longer_than = "8KB"
		older_than  = "12h"
	}

	stage.tenant {
		value = "anonymous"
	}

	forward_to = [module.file.docker_compose.exports.logs_receiver]
}

/********************************************
 * Jaeger for Metrics Logs Traces
 ********************************************/

otelcol.receiver.jaeger "containers" {
	protocols {
		grpc {
			endpoint = "0.0.0.0:14250"
		}

		thrift_http {
			endpoint = "0.0.0.0:14268"
		}

		thrift_binary {
			endpoint = "0.0.0.0:6832"
		}

		thrift_compact {
			endpoint = "0.0.0.0:6831"
		}
	}

	output {
		metrics = [otelcol.processor.batch.containers.input]
		logs    = [otelcol.processor.batch.containers.input]
		traces  = [otelcol.processor.resourcedetection.containers.input]
	}
}

/********************************************
 * Otelcol for Metrics Logs Traces
 ********************************************/

otelcol.receiver.otlp "containers" {
	grpc {
		endpoint = "0.0.0.0:4317"
	}

	http {
		endpoint = "0.0.0.0:4318"
	}

	output {
		metrics = [otelcol.processor.batch.containers.input]
		logs    = [otelcol.processor.batch.containers.input]
		traces  = [
			otelcol.processor.resourcedetection.containers.input,
			otelcol.connector.spanlogs.autologging.input,
		]
	}
}

otelcol.processor.resourcedetection "containers" {
	detectors = ["env"]

	output {
		logs    = [otelcol.processor.attributes.containers.input]
		metrics = [otelcol.processor.attributes.containers.input]
		traces  = [otelcol.processor.attributes.containers.input]
	}
}

otelcol.processor.attributes "containers" {
	// Inserts a new attribute "cluster" to spans where the key doesn't exist.
	action {
		key    = "cluster"
		value  = "docker-compose"
		action = "insert"
	}

	output {
		metrics = [otelcol.processor.transform.add_resource_attributes.input]
		logs    = [otelcol.processor.transform.add_resource_attributes.input]
		traces  = [otelcol.processor.transform.add_resource_attributes.input]
	}
}

otelcol.processor.transform "add_resource_attributes" {
	error_mode = "ignore"

	metric_statements {
		context    = "resource"
		statements = [
			`set(attributes["cluster"], "docker-compose") where attributes["cluster"] == nil`,
		]
	}

	log_statements {
		context    = "resource"
		statements = [
			`set(attributes["pod"], attributes["pod.name"])`,
			`set(attributes["namespace"], attributes["namespace.name"])`,
			`set(attributes["loki.resource.labels"], "pod, namespace, cluster, job")`,
		]
	}

	trace_statements {
		context    = "resource"
		statements = [
			`set(attributes["cluster"], "docker-compose") where attributes["cluster"] == nil`,
		]
	}

	output {
		metrics = [otelcol.processor.filter.containers.input]
		logs    = [otelcol.processor.filter.containers.input]
		traces  = [otelcol.processor.filter.containers.input]
	}
}

otelcol.processor.filter "containers" {
	error_mode = "ignore"

	output {
		metrics = [otelcol.processor.batch.containers.input]
		logs    = [otelcol.processor.batch.containers.input]
		traces  = [otelcol.processor.batch.containers.input]
	}
}

otelcol.processor.batch "containers" {
	send_batch_size     = 16384
	send_batch_max_size = 0
	timeout             = "5s"

	output {
		metrics = [otelcol.processor.memory_limiter.containers.input]
		logs    = [otelcol.processor.memory_limiter.containers.input]
		traces  = [otelcol.processor.memory_limiter.containers.input]
	}
}

otelcol.processor.memory_limiter "containers" {
	check_interval         = "1s"
	limit_percentage       = 50
	spike_limit_percentage = 30

	output {
		metrics = [otelcol.exporter.prometheus.containers.input]
		logs    = [otelcol.exporter.loki.containers.input]
		traces  = [module.file.docker_compose.exports.traces_receiver]
	}
}

otelcol.exporter.prometheus "containers" {
	forward_to = [module.file.docker_compose.exports.metrics_receiver]
}

otelcol.exporter.loki "containers" {
	forward_to = [loki.process.containers.receiver]
}

// The OpenTelemetry spanlog connector processes incoming trace spans and extracts data from them ready
// for logging.
otelcol.connector.spanlogs "autologging" {
	// We only want to output a line for each root span (ie. every single trace), and not for every
	// process or span (outputting a line for every span would be extremely verbose).
	spans     = false
	roots     = true
	processes = false
	// We want to ensure that the following three span attributes are included in the log line, if present.
	span_attributes = ["http.method", "http.target", "http.status_code"]

	// Overrides the default key in the log line to be `traceId`, which is then used by Grafana to
	// identify the trace ID for correlation with the Tempo datasource.
	overrides {
		trace_id_key = "traceId"
	}
	// Send to the OpenTelemetry Loki exporter.
	output {
		logs = [otelcol.exporter.loki.autologging.input]
	}
}

// Simply forwards the incoming OpenTelemetry log format out as a Loki log.
// We need this stage to ensure we can then process the logline as a Loki object.
otelcol.exporter.loki "autologging" {
	forward_to = [loki.process.autologging.receiver]
}

// The Loki processor allows us to accept a correctly formatted Loki log and mutate it into
// a set of fields for output.
loki.process "autologging" {
	// The JSON stage simply extracts the `body` (the actual logline) from the Loki log, ignoring
	// all other fields.
	stage.json {
		expressions = {"body" = ""}
	}
	// The output stage takes the body (the main logline) and uses this as the source for the output
	// logline. In this case, it essentially turns it into logfmt.
	stage.output {
		source = "body"
	}

	forward_to = [loki.process.containers.receiver]
}

/********************************************
 * Profiles
 ********************************************/

pyroscope.scrape "containers" {
	targets = [
		// {"__address__" = "agent:12345", "service_name" = "agent"},
		// {"__address__" = "loki:3100", "service_name" = "loki"},
		// {"__address__" = "grafana:6060", "service_name" = "grafana"},
		// {"__address__" = "mimir:8080", "service_name" = "mimir"},
		{"__address__" = "tempo:3200", "service_name" = "tempo"},
	]

	clustering {
		enabled = true
	}

	profiling_config {
		profile.block {
			enabled = true
		}

		profile.mutex {
			enabled = true
		}

		profile.memory {
			enabled = true
		}

		profile.goroutine {
			enabled = true
		}

		profile.process_cpu {
			enabled = true
		}
	}

	forward_to = [module.file.docker_compose.exports.profiles_receiver]
}
